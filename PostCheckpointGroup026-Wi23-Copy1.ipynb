{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# COGS 118A - Final Group Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Deepansha Singh\n",
    "- Raina Song\n",
    "- Ilya Kogan\n",
    "- Winah Ruiz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Natural language processing (NLP) has been making headlines all over the news in recent months due to the release and gaining popularity of ChatGPT (chatbot developed by OpenAI).\n",
    "To take a closer look at the field of NLP, we plan to implement a sentiment analysis, a NLP technique that is used to determine the text attitude on a scale from -1 to 1 where absolute positive input has score 1, absolute negative input has score of -1, and a score of 0 is for neutral sentiment.\n",
    "In this project we examine how general a sentiment analysis tool can be \n",
    "(e.g. when trained on data on a specific topic how well it can perform on other topics that it didn’t encounter before or general topicless data) \n",
    "by using different sampling techniques \n",
    "(k-fold Cross Validation and Nested Cross Validation) \n",
    "with different classifiers \n",
    "(Support Vector Classifier, k-Nearest Neighbors algorithm, Decision Trees, etc.) \n",
    "and quantifying their performance using various error metrics \n",
    "(precision, recall, or accuracy scores).\n",
    "We will be using social media data from Reddit with sentiment labels to train our models, choose the best one according to an error metric, and then test how well our “best” model performs on a new dataset found on Twitter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In previous studies, the generalisability and interoperability of NLP models in tasks such as fake news detection<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews)<a name=\"fakenews2\"></a>[<sup>[2]</sup>](#fakenews2), hate speech detection<a name=\"hatespeech\"></a>[<sup>[3]</sup>](#hatespeech), and sentiment analysis<a name=\"sentian\"></a>[<sup>[4]</sup>](#sentian) were explored. It was found that generalisability is a challenge for NLP algorithms, as they generalize poorly on new and unseen datasets<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews). However, robust NLP models are desired to perform large-scale, cross-categorical classification tasks on the internet or social media platforms. Some studies suggest that the biases found in models may be due to the small datasets the models are trained on, which make the models prone to overfitting issues<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews). Larger and more diverse datasets are required to reduce biases in trained models. It was suggested that cross-dataset testing is a useful tool to evaluate model generalisability performance realistically<a name=\"hatespeech\"></a>[<sup>[3]</sup>](#hatespeech). Due to such reasons, our project is motivated to explore the generalisability of NLP sentiment analysis models on different datasets pertaining to different subject matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are performing one of the techniques to quantify text, sentiment analysis, using different classifiers. We are then examining how well our model trained on The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate) can perform on Twitter and Reddit Sentimental Analysis Dataset<a name=\"twitterredditsentian\"></a>[<sup>[6]</sup>](#twitterredditsentian) and FIFA World Cup 2022 Tweets<a name=\"fifatweets\"></a>[<sup>[7]</sup>](#fifatweets) by using different error metrics (precision, recall, or accuracy scores) and how does a choice of error metric change what is the best model (i.e. whether we have a completely best model or not). One potential solution to our problem is to use CountVectorizer to do the preprocessing, then extract features using tf-idf frequincies approach after which fit some classifiers (e.g. Support Vector Classifier, k-NN, Decision Trees, and AdaBoost) on The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate). Then, use an error metric, for example, precision or recall and a confusion matrix for visualizations to determine which Classifier gives the best performance on different datasets using, for example, k-fold Cross Validation or Nested Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsyUP33PKbtB"
   },
   "source": [
    "### (pavellexyr) The Reddit Climate Change Dataset: ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
    "\n",
    "There are two .csv files, one containing comments from Reddit on climate change and the other containing posts on climate change. For this project, we will only be using the .csv file with comments as there is already sentiment analysis done on the 4.6 million observations collected.\n",
    "\n",
    "An observation in this dataset consists of:\n",
    "* Type of datapoint (comment)\n",
    "* Unique ID of the comment\n",
    "* Unique ID of the comment’s subreddit\n",
    "* The name of the subreddit the comment was found on\n",
    "* If the comment’s subreddit is NSFW\n",
    "* The timestamp (UTC) of the comment\n",
    "* Permalink to the comment\n",
    "* Body text of the comment\n",
    "* Analyzed sentiment for the comment as a continuous value from [-1, 1]\n",
    "* Comment’s score (votes on Reddit)\n",
    "\n",
    "### (cosmos98) Twitter and Reddit Sentimental analysis Dataset: ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n",
    "\n",
    "There are two .csv files, one containing comments from Reddit (36k observations) and the other containing tweets from Twitter (162k observations). The Twitter dataset was extracted with the focus on tweets people made about the Indian Prime Minister Narendra Modi. The Reddit dataset has no indication of having a specific area or topic they were sourced from. \n",
    "\n",
    "Both datasets only have two variables:\n",
    "* The text of the comment or tweet\n",
    "* The category/sentiment of the text {-1, 0, 1}\n",
    "\n",
    "### (tirendazacademy) FIFA World Cup 2022 Tweets:  ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets\n",
    "\n",
    "There is one .csv file containing tweets regarding the 2022 FIFA World Cup, a dataset of about 22k observations.\n",
    "\n",
    "An observation in this dataset contains:\n",
    "* ID (index) of the observation\n",
    "* Date the tweet was created\n",
    "* Number of likes the tweet had\n",
    "* Source of the tweet (Twitter of iPhone, Twitter for Android)\n",
    "* The body text of the tweet\n",
    "* Sentiment of the tweet as strings: “positive”, “neutral”, or “negative”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3CQ_PPVLPmQ"
   },
   "source": [
    "For all the datasets discussed above, there are only two variables we are concerned with: the text of the comment/tweet and the sentiment the text was already given. Due to some datasets being incredibly large, only the first 100k observations of each dataset will be used. \n",
    "\n",
    "All datasets will undergo further data cleaning. To the best of our ability, we will filter our dataset to only have English detected text using the langdetect library and regex. Other unusable observations (such as rows containing NaN values) will also be excluded. This results in slightly less than the upper limit of 100k observations initially taken from each dataset. In addition, the existing numeric labels for sentiment some datasets may have will be changed to string values of “positive”, “negative”, and “neutral” to be consistent with each other and only have 3 total classes for classification. \n",
    "\n",
    "The full code for cleaning the files used up to this point are in the “COGS118A replacement data cleaning.ipynb” file in this repository. Below will be code snippets, mainly of the functions created to clean the data. For demonstration purposes, these will just be example variable names instead of what was actually used in practice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OuOw8mHvLWCP"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries and global variables to be used for cleaning and limiting collected data to 100k observations at most. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "row_count = 1000\n",
    "max_obv = 100\n",
    "\n",
    "#https://pypi.org/project/langdetect/\n",
    "### uncomment this to install, then comment and restart kernel ###\n",
    "# %%capture\n",
    "# !pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "### regex for more lang checking\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IUoJ6TgYPXR"
   },
   "source": [
    "Taking the first 100k observations of a dataset and putting it into an initial DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_Z8T6rtyMD3n"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gr/fc2cy0q90y552xwg5gr_f8f80000gn/T/ipykernel_4593/100402407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_obv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
     ]
    }
   ],
   "source": [
    "data_text = []\n",
    "data_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('dataset.csv', chunksize=row_count):\n",
    "  if i < max_obv:\n",
    "    data_text += chunk['text'].tolist()\n",
    "    data_sentiment += chunk['sentiment'].tolist()\n",
    "    i += 1\n",
    "  else:\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GSiWZWxsaPgk"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'text': data_text, 'sentiment': data_sentiment})\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvX6BEtraa9i"
   },
   "source": [
    "Function to validate a text body. Valid text will be non-empty strings that are not solely whitespace of at least length 1. Regex and langdetect is used to keep observations that have Latin characters (so that it may be able to filter out text using solely Korean characters for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnV0rg-Xa4KS"
   },
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    if not line:\n",
    "        return np.nan\n",
    "    if line == \"\":\n",
    "        return np.nan\n",
    "    if not bool(line.strip()):\n",
    "        return np.nan\n",
    "    if len(line) < 1:\n",
    "        return np.nan\n",
    "    \n",
    "    if bool(re.match('^(?=.*[a-zA-Z])', line)):\n",
    "        try:\n",
    "            if detect(line) != 'en':\n",
    "                return np.nan\n",
    "        except LangDetectException:\n",
    "            return np.nan\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX9-zm9ja-3b"
   },
   "source": [
    "Function to check if text body is English (detect returns 'en') for the entire dataset. \n",
    "* text_col takes a DataFrame.Series: df['text']\n",
    "* sentiment_col takes a DataFrame.Series: df['sentiment']\n",
    "Returns 3 lists of the same length, truncating the last chunk of observations that are less than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLBYnWoWbwjx"
   },
   "outputs": [],
   "source": [
    "def check_en(text_col, sentiment_col):\n",
    "    en_text = text_col.tolist()\n",
    "    en_sentiment = sentiment_col.tolist()\n",
    "    lang = []\n",
    "    \n",
    "    start = 0\n",
    "    for i in np.arange(row_count, len(en_text), row_count):\n",
    "        #observations <1000 at the end will be lost but impact is negligible\n",
    "        #!!!uncomment print statement below to show progress (recommended)!!!\n",
    "#         print(start, i)\n",
    "        lang += [validate_line(x) for x in en_text[start:i]]\n",
    "        start = i\n",
    "    print(\"Finished English check\")\n",
    "    ### all three return values should be of the same length\n",
    "    return en_text[0:len(lang)], en_sentiment[0:len(lang)], lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjPb4WC_cA3X"
   },
   "source": [
    "Putting the returned lists from check_en into a new DataFrame. df['english'] will be dropped after removing all rows with NaN values (non-English, non valid text bodies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmLd7e1-cWhA"
   },
   "outputs": [],
   "source": [
    "en_data_text, en_data_sentiment, en_data_lang = check_en(df['text'], df['sentiment'])\n",
    "\n",
    "en_df = pd.DataFrame(data={'text': en_data_text, 'sentiment': en_data_sentiment, 'english': en_data_lang})\n",
    "en_df = en_df.dropna()\n",
    "en_df = en_df.drop(columns=['english'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJlVz4iOc56E"
   },
   "source": [
    "Function to change numeric sentiment values into string values, otherwise it will just return the input value (if it was already string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeY9mhQLdELN"
   },
   "outputs": [],
   "source": [
    "def sentiment_to_string(sentiment):\n",
    "    if type(sentiment) == int or type(sentiment) == float:\n",
    "        if sentiment < 0:\n",
    "            return \"negative\"\n",
    "        if sentiment > 0:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1byQ-7UzdK6a"
   },
   "outputs": [],
   "source": [
    "en_df['sentiment'] = en_df['sentiment'].apply(sentiment_to_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mouie_tpq7SD"
   },
   "source": [
    "Further cleaning is done to remove user handles, most links, and most special characteres. Stemming is also done to simplify tokens, which are then put back together as a single body of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TMMx68BkmrlY"
   },
   "outputs": [],
   "source": [
    "#packages needed\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qnA8bVYGm1sU"
   },
   "outputs": [],
   "source": [
    "def further_cleaning(df):\n",
    "  #remove urls\n",
    "  df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', '', x, flags=re.MULTILINE))\n",
    "  #remove user handles\n",
    "  df['text'] = df['text'].apply(lambda x: re.sub(r'@[\\w]*', '', x))\n",
    "  #remove special characters\n",
    "  df['text'] = df['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "  #remove strings less than length 3\n",
    "  df['text'] = df['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "  #tokenizing and stemming\n",
    "  token = df['text'].apply(lambda x : x.split())\n",
    "  token = token.apply(lambda x: [ps.stem(i) for i in x])\n",
    "  \n",
    "  #rejoin tokens\n",
    "  for i in range(len(token)):\n",
    "    token[i] = ' '.join(token[i])\n",
    "  df['text'] = token\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "615OXhUv6x5Y"
   },
   "source": [
    "Final DataFrame after this line with the prefix \"new_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xaBnCPurSVB"
   },
   "outputs": [],
   "source": [
    "new_df = further_cleaning(en_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We have considered 3 different approaches for the proposed solution. For all three approaches, we aim to train different classifiers on a labelled dataset that we mentioned before, and test them on different labelled datasets to achieve the cross-dataset testing as mentioned in the Background section. This way, we can try to find out the generalisability of the models on different datasets and answer our research question. Also, in each approach, different classifiers are evaluated and tested against our metrics and we will select the best model in each approach. For approaches 1 and 2, we are looking at supervised machine learning methods for the classifiers. And for approach 3, which is beyond the scope of the class (but if we have extra time) we will also do some exploration for some deep learning package’s accuracy on the dataset.\n",
    "<br>\n",
    "\n",
    "**Approach #1 : Supervised ML, using scikit packages to take care of BOTH the data preprocessing and also the NLP feature extraction.**\n",
    "\n",
    "All the preprocessing will be taken care of by the CountVectorizer.\n",
    "Please note that we are following this tutorial from the Scikit documentation <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1). Since we haven’t previously worked with natural language processing problems in class before, we are  following this tutorial’s methods for feature extraction. Additionally, we did a lot of thorough, extensive research on various approaches for extracting features for text input. Some features <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1)<a name=\"featextract\"></a>[<sup>[9]</sup>](#featextract) we would be looking at are bag-of-words feature, tf-idf frequencies <a name=\"wiki\"></a>[<sup>[10]</sup>](#wiki), and word embeddings <a name=\"mediium\"></a>[<sup>[11]</sup>](#medium). \n",
    "\n",
    "Specifically, for the tf-idf frequencies, based on the research we did <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1), <a name=\"wiki\"></a>[<sup>[12]</sup>](#wiki), tf-idf is a great way to see how important each of these words in the text are; tf-idf stands for term frequency-inverse document frequency. For tf-idf, we need to use the \"CountVectorizer\" object from scikit <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1) to get the frequencies and then we can use that for this tf-idf computation. While the tf part is more focused on seeing how frequently each of these words are in our training set, the idf portion of the computation is more focused around information theory aspect and assessing the relevance of the word <a name=\"wiki\"></a>[<sup>[10]</sup>](#wiki).\n",
    "\n",
    "After extracting all the various features, the next step would be to run the various classifiers <a name=\"scikit2\"></a>[<sup>[12]</sup>](#scikit2) on the training dataset. Specifically, these are the following classifiers we are planning on running on these features as discussed in class: Support Vector Classifier, K-nearest neighbors, Ada Boost, and Decision trees.\n",
    "Some new classifiers, which we haven’t learned in class yet, which we are also planning to investigate in further detail are neural net, Naive Bayes, etc. <a name=\"scikit2\"></a>[<sup>[12]</sup>](#scikit2). We will be comparing these classifiers on different datasets and select the best model based on our metrics. \n",
    "\n",
    "In addition, for the training sampling techniques, we will be using the following to do comparisons between the different methods that we learned in class: K-fold validation with training, with training/validation/testing, and Nested cross validation. \n",
    "<br>\n",
    "\n",
    "**Approach #2: Supervised ML, doing data preprocessing and above feature extraction from scratch.**\n",
    "\n",
    "The steps for this approach will be similar to the first approach, however for the data preprocessing we will be doing everything from scratch instead of using CountVectorizer. \n",
    "Specifically, these are some of the data preprocessing techniques that we have researched about <a name=\"empstudy\"></a>[<sup>[13]</sup>](#empstudy)<a name=\"featextractreview\"></a>[<sup>[14]</sup>](#featextractreview) for natural language processing sub-field of machine learning are lemmatization/stemming, tokenization, and looking at part of speech for text.\n",
    "Additionally, we will be looking at more research papers and trying to see more data preprocessing techniques for text input data.\n",
    "We will again compare and contrast each classifier, and select the best model in this approach.\n",
    "<br>\n",
    "\n",
    "**Approach #3 (Above & Beyond, Extra work outside of class algorithms): Running on pre-trained neural networks.**\n",
    "\n",
    "Along with supervised ML approaches, there has been a lot of extensive, thorough research being done in deep learning space for Natural Language Processing. Google’s BERT and Open AI’s GPT-3 <a name=\"top8\"></a>[<sup>[15]</sup>](#top8) are just two of the popular NLP deep learning models out there. \n",
    "We will be loading these pretrained models in the Pytorch library. This is a library we haven’t learned in class, but is widely used in deep learning research problems. Specifically, this is new Machine learning content, in deep learning and transformers space, and we spent time understanding these really cool new ML concepts (links below). \n",
    "After performing the above tasks for the supervised machine learning component of the project, we will also be comparing the accuracy of this model to various pretrained models from HuggingFace for sentiment analysis, and see which model provides best accuracy, specifically BERT pretrained model <a name=\"huggingface1\"></a>[<sup>[16]</sup>](#huggingface1) and Chat GPT-2 pretrained model <a name=\"huggingface2\"></a>[<sup>[17]</sup>](#huggingface2).\n",
    "\n",
    "If time permits, we will also try to train a neural network from scratch for sentiment analysis and do further research on research papers in this space. \n",
    "\n",
    "For all these different classification models, after we selected the best model by observing the results they produce on the labelled datasets, we will test them against an unlabelled dataset and see how accurate they seem on the unlabelled dataset compared to the labelled ones they were trained and tested on. In such a way, we can see not only the cross-dataset testing results on the generalisability of models we selected, and also the predictability of the models when they don't have labelled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will be using various different error metrics such as precision, recall, and overall accuracy score to evaluate how well those classifiers can generalize in sentiment analysis given different subject matter, and if there is a significant difference in performance between each classifier. Precision and recall will also be used to judge how sensitive our classifiers are to negative and positive sentiments, providing insight as to what pitfalls they are subject to in the human language such as sarcasm.\n",
    "\n",
    "$$Precision = {True Positives \\over Predicted Positives}$$\n",
    "$$Recall = {True Positives \\over Actually Positive}$$\n",
    "$$Overall Accuracy = {True Positives + True Negatives \\over Total Predictions Made}$$\n",
    "\n",
    "We will also use a confusion matrix<a name=\"confusionMatrix\"></a>[<sup>[18]</sup>](#confusionMatrix) to visually represent True Positives, False Negatives, False Positives, and True Negatives, since they are the basis using which precision, recall, and overall accuracy are evaluated. A general Confusion Matrix looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    |Predicted Positive(PP)| Predicted Negative(PN)|\n",
    "| ------------------ | -------------------- | --------------------- |\n",
    "| Actual Positive(P) |    True Positives    |    False Negatives    |\n",
    "| Actual Negative(N) |    False Positives   |    True Negatives     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Model Using The Reddit Climate Change Dataset and KNeighborsClassifier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Yeah but what the above commenter is saying is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Any comparison of efficiency between solar and...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm honestly waiting for climate change and th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Not just Sacramento. It's actually happening a...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I think climate change tends to get some peopl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97744</th>\n",
       "      <td>97995</td>\n",
       "      <td>It’s almost like climate change is real.   Huh.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97745</th>\n",
       "      <td>97996</td>\n",
       "      <td>I'm not sure I agree. More Americans are consi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97746</th>\n",
       "      <td>97997</td>\n",
       "      <td>If 40 billion could fix climate change why did...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97747</th>\n",
       "      <td>97998</td>\n",
       "      <td>There are a lot of answers to climate change, ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97748</th>\n",
       "      <td>97999</td>\n",
       "      <td>He’s full of shit. It’s outlined and debunked ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97749 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text sentiment\n",
       "0               0  Yeah but what the above commenter is saying is...  positive\n",
       "1               1  Any comparison of efficiency between solar and...  negative\n",
       "2               2  I'm honestly waiting for climate change and th...  negative\n",
       "3               3  Not just Sacramento. It's actually happening a...   neutral\n",
       "4               4  I think climate change tends to get some peopl...  positive\n",
       "...           ...                                                ...       ...\n",
       "97744       97995    It’s almost like climate change is real.   Huh.  positive\n",
       "97745       97996  I'm not sure I agree. More Americans are consi...  positive\n",
       "97746       97997  If 40 billion could fix climate change why did...   neutral\n",
       "97747       97998  There are a lot of answers to climate change, ...   neutral\n",
       "97748       97999  He’s full of shit. It’s outlined and debunked ...  negative\n",
       "\n",
       "[97749 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_climate_df = pd.read_csv('en_climate_df.csv')\n",
    "en_climate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = en_climate_df['text']\n",
    "Y = en_climate_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'neutral', 'negative', ..., 'positive', 'positive',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with original test set\n",
    "X_new_counts = countVectorizer.transform(X_test)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5167519181585678"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get accuracy on the original test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 215, in fit\n    return self._fit(X, y)\n           ^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py\", line 893, in __array__\n    return np.asarray(self._values, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: \"* Expensive, well-organized, and non-stigmatized prostitution. With better living standards there are fewer people forced into prostitution, thus decreasing the supply and giving better bargaining power to the workers. No more prostitution ghetto in poor cities and countries alongside their protection rackets and drug dealers. A character can be seen working a decidedly middle-class job like a ballet instructor, one of their students's mom is publicly known as being a prostitute, treated like any other parent.\\n\\n* Far less violent crime. When most criminal gangs can actually do honest work for a decent living, the stakes are lower and they are no longer willing to risk jail just to keep their territory. They will still steal and smuggle, but when confronted by civilians or outnumbered police, they just disengage instead of resorting to beatings/shoot out. The police might mention it's been years since they were forced to use their baton.\\n\\n* Way faster tech development. Even if the Contingency won't outright research cold fusion or eliminate cancer, nearly twice the world population and better education standards mean more researchers and potential market. Climate change and pandemics are kept at bay, and what we'd consider futuristic industries like space and robotics are perfectly mundane. A defeat in war might merely mean figuring out the robotic army factory has been surrounded by the enemy with no human blood ever spilled, then filling red tape on where the loser's going to be exiled, a perfectly habitable artificial island with more amenities than our New York or a Moon station where people's biggest annoyance is solar storm delaying season premiere of their favorite show.\"\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 215, in fit\n    return self._fit(X, y)\n           ^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py\", line 893, in __array__\n    return np.asarray(self._values, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Yeah but what the above commenter is saying is their base doesn’t want any of that. They detest all of those things, even the small gradual changes. Investing in nuclear energy is a tacit acknowledgement of man made climate change. Any acknowledgement or concession and they will be primaried out in a minute'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ilyakogan/Desktop/Group026-Wi23/PostCheckpointGroup026-Wi23-Copy1.ipynb Cell 41\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ilyakogan/Desktop/Group026-Wi23/PostCheckpointGroup026-Wi23-Copy1.ipynb#Y132sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m k_values:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ilyakogan/Desktop/Group026-Wi23/PostCheckpointGroup026-Wi23-Copy1.ipynb#Y132sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     knn \u001b[39m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[39m=\u001b[39mk)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ilyakogan/Desktop/Group026-Wi23/PostCheckpointGroup026-Wi23-Copy1.ipynb#Y132sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     scores \u001b[39m=\u001b[39m cross_val_score(knn, X, Y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ilyakogan/Desktop/Group026-Wi23/PostCheckpointGroup026-Wi23-Copy1.ipynb#Y132sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     cv_scores\u001b[39m.\u001b[39mappend(scores\u001b[39m.\u001b[39mmean())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ilyakogan/Desktop/Group026-Wi23/PostCheckpointGroup026-Wi23-Copy1.ipynb#Y132sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#Choose the best k value\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 215, in fit\n    return self._fit(X, y)\n           ^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py\", line 893, in __array__\n    return np.asarray(self._values, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: \"* Expensive, well-organized, and non-stigmatized prostitution. With better living standards there are fewer people forced into prostitution, thus decreasing the supply and giving better bargaining power to the workers. No more prostitution ghetto in poor cities and countries alongside their protection rackets and drug dealers. A character can be seen working a decidedly middle-class job like a ballet instructor, one of their students's mom is publicly known as being a prostitute, treated like any other parent.\\n\\n* Far less violent crime. When most criminal gangs can actually do honest work for a decent living, the stakes are lower and they are no longer willing to risk jail just to keep their territory. They will still steal and smuggle, but when confronted by civilians or outnumbered police, they just disengage instead of resorting to beatings/shoot out. The police might mention it's been years since they were forced to use their baton.\\n\\n* Way faster tech development. Even if the Contingency won't outright research cold fusion or eliminate cancer, nearly twice the world population and better education standards mean more researchers and potential market. Climate change and pandemics are kept at bay, and what we'd consider futuristic industries like space and robotics are perfectly mundane. A defeat in war might merely mean figuring out the robotic army factory has been surrounded by the enemy with no human blood ever spilled, then filling red tape on where the loser's going to be exiled, a perfectly habitable artificial island with more amenities than our New York or a Moon station where people's biggest annoyance is solar storm delaying season premiere of their favorite show.\"\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_classification.py\", line 215, in fit\n    return self._fit(X, y)\n           ^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neighbors/_base.py\", line 454, in _fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py\", line 893, in __array__\n    return np.asarray(self._values, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Yeah but what the above commenter is saying is their base doesn’t want any of that. They detest all of those things, even the small gradual changes. Investing in nuclear energy is a tacit acknowledgement of man made climate change. Any acknowledgement or concession and they will be primaried out in a minute'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#array of first 50 odd integers\n",
    "k_values = list(range(1, 100, 2))\n",
    "\n",
    "#data preprocessing\n",
    "X_counts = countVectorizer.transform(X)\n",
    "X_tfidf = tf_transformer.transform(X_new_counts)\n",
    "\n",
    "#cross-validation w/ 5 folds to evaluate the performance of kNN model for each value of k\n",
    "cv_scores = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_tfidf, Y, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "    \n",
    "#Choose the best k value\n",
    "best_k = k_values[np.argmax(cv_scores)]\n",
    "print(\"Best k value: {}\".format(best_k))\n",
    "\n",
    "#line plot to visualize the accuracy scores for each k\n",
    "plt.plot(k_values, accuracy_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting confusion matrix, actual|pred is the order for confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "ax= plt.subplot()\n",
    "cm1 = confusion_matrix(Y_test, predicted)\n",
    "disp = ConfusionMatrixDisplay(cm1)\n",
    "disp.plot(ax=ax)\n",
    "ax.xaxis.set_ticklabels(['-1','0', '1']); ax.yaxis.set_ticklabels(['-1','0', '1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing The Model Using Different Datasets To Get Prediction Accuracies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         family mormon have never tried explain them t...\n",
       "1        buddhism has very much lot compatible with chr...\n",
       "2        seriously don say thing first all they won get...\n",
       "3        what you have learned yours and only yours wha...\n",
       "4        for your own benefit you may want read living ...\n",
       "                               ...                        \n",
       "31643            coincidentally that how randia works too \n",
       "31644               here screen cap his live telecast jpg \n",
       "31645              shit forgot today date take upvote sir \n",
       "31646     fell for this good one also shows overtly opt...\n",
       "31647            thought now going ban 2000 rupee note lol\n",
       "Name: text, Length: 31648, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with cosmos redit sentiment data\n",
    "cosmos_reddit_test = pd.read_csv('en_cosmos_reddit_df.csv')\n",
    "cosmos_reddit_test_data = cosmos_reddit_test['text']\n",
    "cosmos_reddit_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'negative', ..., 'negative', 'positive',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_counts = countVectorizer.transform(cosmos_reddit_test_data)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  12515\n",
      "passed:  0\n",
      "accuracy:  0.39544362992922144\n"
     ]
    }
   ],
   "source": [
    "cosmos_reddit_test_sentiment = cosmos_reddit_test['sentiment']\n",
    "accuracy_score(cosmos_reddit_test_sentiment, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        when modi promised “minimum government maximum...\n",
       "1        talk all the nonsense and continue all the dra...\n",
       "2        what did just say vote for modi  welcome bjp t...\n",
       "3        asking his supporters prefix chowkidar their n...\n",
       "4        answer who among these the most powerful world...\n",
       "                               ...                        \n",
       "91953                 india cant survive another term modi\n",
       "91954    modi hands down indians are too influenced bol...\n",
       "91955    rajdeep known congress supporters from day one...\n",
       "91956                       its bcoz they hate modi thats \n",
       "91957    narendra modi begin campaign trail address pub...\n",
       "Name: text, Length: 91958, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with cosmos twitter sentiment data\n",
    "cosmos_twitter_test = pd.read_csv('en_cosmos_twitter_df.csv')\n",
    "cosmos_twitter_test_data = cosmos_twitter_test['text']\n",
    "cosmos_twitter_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'negative', ..., 'positive', 'negative',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_counts = countVectorizer.transform(cosmos_twitter_test_data)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  32583\n",
      "passed:  0\n",
      "accuracy:  0.35432480045238046\n"
     ]
    }
   ],
   "source": [
    "cosmos_twitter_test_sentiment = cosmos_twitter_test['sentiment']\n",
    "accuracy_score(cosmos_twitter_test_sentiment, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        What are we drinking today @TucanTribe \\n@MadB...\n",
       "1        Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
       "2        Worth reading while watching #WorldCup2022 htt...\n",
       "3        Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
       "4        If the BBC cares so much about human rights, h...\n",
       "                               ...                        \n",
       "21793    Leave #FIFA alone! Let the #soccer prevail and...\n",
       "21794    Three stars on this logo after WC\\n#WorldCup20...\n",
       "21795    What is really sickening is the west trying to...\n",
       "21796    Messi’s last World Cup! I am going with Argent...\n",
       "21797    Who will win the World Cup 2022?\\n🏆⚽️\\n\\n@FIFA...\n",
       "Name: text, Length: 21798, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with fifa twitter data\n",
    "fifa_test = pd.read_csv('en_fifa_df.csv')\n",
    "fifa_test_data = fifa_test['text']\n",
    "fifa_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'positive', ..., 'positive', 'positive',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_counts = countVectorizer.transform(fifa_test_data)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  7793\n",
      "passed:  0\n",
      "accuracy:  0.3575098632902101\n"
     ]
    }
   ],
   "source": [
    "fifa_test_sentiment = fifa_test['sentiment']\n",
    "accuracy_score(fifa_test_sentiment, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining datasets together since the accuracy wasn't too high, and training on combined dataset instead. Also trying a multitude of various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verifying the lengths of the data\n",
      "97749\n",
      "31648\n",
      "129397\n",
      "\n",
      "\n",
      "verifying the lengths of the y value arrays\n",
      "97749\n",
      "31648\n",
      "129397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'neutral', 'positive', 'neutral', 'neutral', 'negative',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate_data = en_climate_df['text']\n",
    "cosmos_reddit = pd.read_csv('en_cosmos_reddit_df.csv')\n",
    "cosmos_reddit_data = cosmos_reddit_test['text']\n",
    "concatenated_data = pd.concat([climate_data, cosmos_reddit_data])\n",
    "\n",
    "# checking the lengths\n",
    "print(\"verifying the lengths of the data\")\n",
    "print(len(climate_data))\n",
    "print(len(cosmos_reddit))\n",
    "print(len(concatenated_data))\n",
    "\n",
    "climate_y = en_climate_df['sentiment']\n",
    "cosmos_reddit_y = cosmos_reddit['sentiment']\n",
    "concatenated_y = pd.concat([climate_y, cosmos_reddit_y])\n",
    "\n",
    "# checking the lengths\n",
    "print(\"\\n\")\n",
    "print(\"verifying the lengths of the y value arrays\")\n",
    "print(len(climate_y))\n",
    "print(len(cosmos_reddit_y))\n",
    "print(len(concatenated_y))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(concatenated_data, concatenated_y, random_state=15, train_size=0.6)\n",
    "# df = pd.DataFrame(data={'text': data_text, 'sentiment': data_sentiment})\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)\n",
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  22603\n",
      "passed:  0\n",
      "accuracy:  0.43669699955563285\n"
     ]
    }
   ],
   "source": [
    "# Testing on original test set\n",
    "X_new_counts = countVectorizer.transform(X_test)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted\n",
    "Y_test=Y_test.tolist()\n",
    "# Get accuracy on the original test set\n",
    "correct = 0\n",
    "passed = 0\n",
    "# print(len(predicted))\n",
    "for i in range(len(predicted)):\n",
    "#     print(i)\n",
    "    try:\n",
    "        if Y_test[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    except KeyError:\n",
    "        passed += 1\n",
    "        pass\n",
    "print(\"correct: \", correct)\n",
    "print(\"passed: \", passed)\n",
    "print(\"accuracy: \", correct/(len(predicted)-passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on FIFA dataset (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying part 3, Using pretrained HuggingFace model to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/resolve/main/config.json from cache at /Users/deepansha/.cache/huggingface/transformers/034fd09e9530137fb6e6c042529972a92619fb02df8b40e7a4cfc50090943c46.ba567638740ab836f48b011b60649b828abc78b1aafda381bf9ac862d58d1ff5\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/resolve/main/pytorch_model.bin from cache at /Users/deepansha/.cache/huggingface/transformers/46eaf2275c70a984feed87be9019d2c427e10e13e4498d391bedffc0c61b4991.e408f6b4bc4b9a45898edc7d681a8b605aef8b68f4174e6746c2d2b0c2105aee\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at pysentimiento/robertuito-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/resolve/main/tokenizer.json from cache at /Users/deepansha/.cache/huggingface/transformers/47dd2d3180a6186d30715516321375322e3a84d5e4656762e083091bbb5d5dc4.eec39d7c1045547ef845896e88798d42cff83c5a9e959ffcde0e93fd6320ae6b\n",
      "loading file https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/resolve/main/special_tokens_map.json from cache at /Users/deepansha/.cache/huggingface/transformers/25e0e805456d2786a12b70b86278c6e839d19958cb4f541ee1f78621140098f7.adf9ca26cf9cb894c06d81ab14f4ad5657962a0f38dbf8000649a29331641b87\n",
      "loading file https://huggingface.co/pysentimiento/robertuito-sentiment-analysis/resolve/main/tokenizer_config.json from cache at /Users/deepansha/.cache/huggingface/transformers/4fdf9a5a8e0a6023e1a9cdef62921158c3db9545e73f8dc0f46340a21bbb64d5.036a79de0b81e048666ccb643b07ce1c7ddc1b57039cb6443e2cfd08778ed54c\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58649 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 58649\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pysentimiento \n",
    "# for \n",
    "\n",
    "# testing on original climate dataset first \n",
    "X = en_climate_df['text']\n",
    "Y = en_climate_df['sentiment']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=0.6)\n",
    "\n",
    "train_predictions = pysentimiento.create_analyzer(task=\"sentiment\", lang=\"es\").predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalyzerOutput(output=NEG, probas={NEG: 0.926, NEU: 0.056, POS: 0.018})\n",
      "NEG\n"
     ]
    }
   ],
   "source": [
    "print(train_predictions[0])\n",
    "x = train_predictions[0].output\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xaBnCPurSVB"
   },
   "outputs": [],
   "source": [
    "train_predictions = [x.output for x in train_predictions]\n",
    "train_predictions = [\"positive\" if x == 'POS' else \"negative\" if x == 'NEG' else \"neutral\" for x in train_predictions]\n",
    "train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on trainset\n",
    "correct = 0\n",
    "passed = 0\n",
    "for i in range(len(predicted)):\n",
    "    try:\n",
    "        if Y_test[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    except KeyError:\n",
    "        passed += 1\n",
    "        pass\n",
    "print(\"accuracy: \", correct/(len(predicted)-passed))\n",
    "\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on testset\n",
    "correct = 0\n",
    "passed = 0\n",
    "for i in range(len(predicted)):\n",
    "    try:\n",
    "        if Y_test[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    except KeyError:\n",
    "        passed += 1\n",
    "        pass\n",
    "print(\"accuracy: \", correct/(len(predicted)-passed))\n",
    "\n",
    "print(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our premiliary results shown above, we can see the base model we trained on the the Reddit Climate Change Dataset with the KNeighborsClassifier has a test accuracy* (on the original dataset training data) of 0.4284 or 42.84%.\n",
    "\n",
    "If we use other datasets to test the model, we got 0.3956 or 39.57% for Reddit Sentiment Analysis Dataset, 0.3542 or 35.42% for Twitter Sentiment Analysis Dataset, and 0.3578 or 35.78% for FIFA Tweets Dataset. The results are lower than the accuracy on the original testing data.\n",
    "\n",
    "Such a result was expected, as the trained model has generalisation error on unseen data, and compared to data from the original dataset, the error on different datasets was expected to be larger. However, we can see that the general prediction accuracy or performance of the trained model is quite low, even on the original dataset test data. This might mean that our model is not optimal and there could be other options to explore and improve the accuracy.\n",
    "\n",
    "Thus, we may try to use a different algorithm other than the KNeighborsClassifier to train the model, to compare its performance with this model. In addition, we also plan to train the model on different datasets, to see if there is a difference in models trained with different training data. \n",
    "\n",
    "**The premiliary result accuracies were calculated using \"number of correct prediction/number of all predictions\" to get quick feedback. We will use other evaluation metrics as described in the above section in our final evaluation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the datasets all coming from real people collected through social media, there are ethical concerns over their consent in being part of this project as points of data, even with their words and personal information accessible through the media the data is extracted from (Twitter and Reddit).\n",
    "Proper data cleaning and handling would require giving those people as much anonymity that is possible to be given, such as removing their usernames/handles and even the permalink/id to avoid being traced back to a user or community.\n",
    "\n",
    "The goal of this project is to see if, given the labels already assigned in some datasets, how well can a model be trained on them to label other datasets?\n",
    "As the data comes from a sample of people in the population of those respective media for certain topics, the text content may be biased towards/against certain topics or communities.\n",
    "Some datasets have already included sentiment labels to indicate if they are positive, negative, or neutral.\n",
    "Using this data in training may contribute to the model’s perceived connotation of those and related texts in unlabeled datasets.\n",
    "Conclusions drawn from those results may be harmful to a degree regardless of actual intent or content because of the nature of applying sentiment analysis.\n",
    "This project and those that are working with it do not show any personal support or opposition for any of the data gathered, and to keep the integrity of the project itself, all usable content (those in the English language) can be used without censorship as to not influence the direction of the model toward our personal preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Communication channel is FB Messenger.\n",
    "* Notify of any changes of plans in the communication channel as soon as possible.\n",
    "* Confirm or reschedule meeting times for upcoming week if you know that you will not be able to meet at the specified time. \n",
    "* Come prepared to meetings with completed tasks.\n",
    "* Ask for assistance anytime you feel like you need it.\n",
    "* Don’t be afraid to pair up to learn some new skill together or help out one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date and Time | Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|\n",
    "| 2/22 @2 PM |  Brainstormed topics/questions (all)  | Determined best form of communication; Discussed and decided on final project topic; discussed hypothesis; began background research | \n",
    "| 3/3 @5 PM  | Read reviews of our project  | Tailor our project based on what we learned from peer reviews; discuss further steps   |\n",
    "| 3/8 @8 PM  | Prepare your part for checkpoint submission, Import and wrangle Data, do some EDA | Resolve any issues, submit Checkpoint; Review/edit wrangling; discuss further steps   |\n",
    "| 3/12 @8 PM | Be up to date, bring in any concerns or what still needs to be done. | Review of what is done so far. Discuss further steps |\n",
    "| 3/16 @6 PM | Complete your part(s). Prepare any questions and/or concerns. Last chance to change something| Discuss last steps towards project completion. Distribute work to complete the project |\n",
    "| 3/22 @2 PM | Finish all the gaps, fill in all the nits and bits  | Turn in the Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"fakenews\"></a>1.[^](#fakenews): LHoy, N., &amp; Koulouri, T. (2022). Exploring the generalisability of fake news detection models. *2022 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata55660.2022.10020583<br> \n",
    "<a name=\"fakenews2\"></a>2.[^](#fakenews2): Blackledge, C., &amp; Atapour-Abarghouei, A. (2021). Transforming fake news: Robust generalisable news classification using Transformers. *2021 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata52589.2021.9671970<br>\n",
    "<a name=\"hatespeech\"></a>3.[^](#hatespeech): Yin W, Zubiaga A. 2021. Towards generalisable hate speech detection: a review on obstacles and solutions. *PeerJ Computer Science* 7:e598 https://doi.org/10.7717/peerj-cs.598<br>\n",
    "<a name=\"sentian\"></a>4.[^](#sentian): Moore, A., & Rayson, P. (2018). Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for target dependent sentiment analysis. *arXiv preprint arXiv:1806.05219*<br>\n",
    "<a name=\"redditclimate\"></a>5.[^](#redditclimate): https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset<br>\n",
    "<a name=\"twitterredditsentian\"></a>6.[^](#twitterredditsentian): https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset<br>\n",
    "<a name=\"fifatweets\"></a>7.[^](#fifatweets): https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets<br>\n",
    "<a name=\"scikit1\"></a>8.[^](#scikit1): https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#exercise-2-sentiment-analysis-on-movie-reviews<br>\n",
    "<a name=\"featextract\"></a>9.[^](#featextract): Zareapoor, M., &amp; K. R, S. (2015). Feature extraction or feature selection for text classification: A case study on phishing email detection. *International Journal of Information Engineering and Electronic Business*, 7(2), 60–65. https://doi.org/10.5815/ijieeb.2015.02.08<br>\n",
    "<a name=\"wiki\"></a>10.[^](#wiki): https://en.wikipedia.org/wiki/Tf%E2%80%93idf<br>\n",
    "<a name=\"medium\"></a>11.[^](#medium): https://medium.com/geekculture/text-feature-extraction-3-3-word-embeddings-model-e98f3d270dce<br>\n",
    "<a name=\"scikit2\"></a>12.[^](#scikit2): https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html<br>\n",
    "<a name=\"empstudy\"></a>13.[^](#empstudy): Sun, X., Liu, X., Hu, J., &amp; Zhu, J. (2014). Empirical studies on the NLP techniques for source code data preprocessing. *Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies*. https://doi.org/10.1145/2627508.2627514<br>\n",
    "<a name=\"featextractreview\"></a>14.[^](#featextractreview): Asghar, M. Z., Khan, A., Ahmad, S., & Kundi, F. M. (2014). A review of feature extraction in sentiment analysis. *Journal of Basic and Applied Scientific Research*, 4(3), 181-186.<br>\n",
    "<a name=\"top8\"></a>15.[^](#top8): https://analyticsindiamag.com/top-8-pre-trained-nlp-models-developers-must-know/<br>\n",
    "<a name=\"huggingface1\"></a>16.[^](#huggingface1): https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis?text=Yay%21%21<br>\n",
    "<a name=\"huggingface2\"></a>17.[^](#huggingface2): https://huggingface.co/michelecafagna26/gpt2-medium-finetuned-sst2-sentiment?text=yayy%21%21<br>\n",
    "<a name=\"confusionMatrix\"></a>18.[^](#confusionMatrix): https://en.wikipedia.org/wiki/Confusion_matrix<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
