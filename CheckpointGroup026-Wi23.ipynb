{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Deepansha Singh\n",
    "- Raina Song\n",
    "- Ilya Kogan\n",
    "- Winah Ruiz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Natural language processing (NLP) has been making headlines all over the news in recent months due to the release and gaining popularity of ChatGPT (chatbot developed by OpenAI).\n",
    "To take a closer look at the field of NLP, we plan to implement a sentiment analysis, a NLP technique that is used to determine the text attitude on a scale from -1 to 1 where absolute positive input has score 1, absolute negative input has score of -1, and a score of 0 is for neutral sentiment.\n",
    "In this project we examine how general a sentiment analysis tool can be \n",
    "(e.g. when trained on data on a specific topic how well it can perform on other topics that it didn’t encounter before or general topicless data) \n",
    "by using different sampling techniques \n",
    "(k-fold Cross Validation and Nested Cross Validation) \n",
    "with different classifiers \n",
    "(Support Vector Classifier, k-Nearest Neighbors algorithm, Decision Trees, etc.) \n",
    "and quantifying their performance using various error metrics \n",
    "(precision, recall, or accuracy scores).\n",
    "We will be using social media data from Reddit with sentiment labels to train our models, choose the best one according to an error metric, and then test how well our “best” model performs on a new dataset found on Twitter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In previous studies, the generalisability and interoperability of NLP models in tasks such as fake news detection<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews)<a name=\"fakenews2\"></a>[<sup>[2]</sup>](#fakenews2), hate speech detection<a name=\"hatespeech\"></a>[<sup>[3]</sup>](#hatespeech), and sentiment analysis<a name=\"sentian\"></a>[<sup>[4]</sup>](#sentian) were explored. It was found that generalisability is a challenge for NLP algorithms, as they generalize poorly on new and unseen datasets<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews). However, robust NLP models are desired to perform large-scale, cross-categorical classification tasks on the internet or social media platforms. Some studies suggest that the biases found in models may be due to the small datasets the models are trained on, which make the models prone to overfitting issues<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews). Larger and more diverse datasets are required to reduce biases in trained models. It was suggested that cross-dataset testing is a useful tool to evaluate model generalisability performance realistically<a name=\"hatespeech\"></a>[<sup>[3]</sup>](#hatespeech). Due to such reasons, our project is motivated to explore the generalisability of NLP sentiment analysis models on different datasets pertaining to different subject matter.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are performing one of the techniques to quantify text, sentiment analysis, using different classifiers. We are then examining how well our model trained on The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate) can perform on Twitter and Reddit Sentimental Analysis Dataset<a name=\"twitterredditsentian\"></a>[<sup>[6]</sup>](#twitterredditsentian) and FIFA World Cup 2022 Tweets<a name=\"fifatweets\"></a>[<sup>[7]</sup>](#fifatweets) by using different error metrics (precision, recall, or accuracy scores) and how does a choice of error metric change what is the best model (i.e. whether we have a completely best model or not). One potential solution to our problem is to use CountVectorizer to do the preprocessing, then extract features using tf-idf frequincies approach after which fit some classifiers (e.g. Support Vector Classifier, k-NN, Decision Trees, and AdaBoost) on The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate). Then, use an error metric, for example, precision and recall and a confusion matrix to determine which Classifier gives the best performance on different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsyUP33PKbtB"
   },
   "source": [
    "### (pavellexyr) The Reddit Climate Change Dataset: ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
    "\n",
    "There are two .csv files, one containing comments from Reddit on climate change and the other containing posts on climate change. For this project, we will only be using the .csv file with comments as there is already sentiment analysis done on the 4.6 million observations collected.\n",
    "\n",
    "An observation in this dataset consists of:\n",
    "* Type of datapoint (comment)\n",
    "* Unique ID of the comment\n",
    "* Unique ID of the comment’s subreddit\n",
    "* The name of the subreddit the comment was found on\n",
    "* If the comment’s subreddit is NSFW\n",
    "* The timestamp (UTC) of the comment\n",
    "* Permalink to the comment\n",
    "* Body text of the comment\n",
    "* Analyzed sentiment for the comment as a continuous value from [-1, 1]\n",
    "* Comment’s score (votes on Reddit)\n",
    "\n",
    "### (cosmos98) Twitter and Reddit Sentimental analysis Dataset: ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n",
    "\n",
    "There are two .csv files, one containing comments from Reddit (36k observations) and the other containing tweets from Twitter (162k observations). The Twitter dataset was extracted with the focus on tweets people made about the Indian Prime Minister Narendra Modi. The Reddit dataset has no indication of having a specific area or topic they were sourced from. \n",
    "\n",
    "Both datasets only have two variables:\n",
    "* The text of the comment or tweet\n",
    "* The category/sentiment of the text {-1, 0, 1}\n",
    "\n",
    "### (tirendazacademy) FIFA World Cup 2022 Tweets:  ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets\n",
    "\n",
    "There is one .csv file containing tweets regarding the 2022 FIFA World Cup, a dataset of about 22k observations.\n",
    "\n",
    "An observation in this dataset contains:\n",
    "* ID (index) of the observation\n",
    "* Date the tweet was created\n",
    "* Number of likes the tweet had\n",
    "* Source of the tweet (Twitter of iPhone, Twitter for Android)\n",
    "* The body text of the tweet\n",
    "* Sentiment of the tweet as strings: “positive”, “neutral”, or “negative”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3CQ_PPVLPmQ"
   },
   "source": [
    "For all the datasets discussed above, there are only two variables we are concerned with: the text of the comment/tweet and the sentiment the text was already given. Due to some datasets being incredibly large, only the first 100k observations of each dataset will be used. \n",
    "\n",
    "All datasets will undergo further data cleaning. To the best of our ability, we will filter our dataset to only have English detected text using the langdetect library and regex. Other unusable observations (such as rows containing NaN values) will also be excluded. This results in slightly less than the upper limit of 100k observations initially taken from each dataset. In addition, the existing numeric labels for sentiment some datasets may have will be changed to string values of “positive”, “negative”, and “neutral” to be consistent with each other and only have 3 total classes for classification. \n",
    "\n",
    "The full code for cleaning the files used up to this point are in the “COGS118A replacement data cleaning.ipynb” file in this repository. Below will be code snippets, mainly of the functions created to clean the data. For demonstration purposes, these will just be example variable names instead of what was actually used in practice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OuOw8mHvLWCP"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries and global variables to be used for cleaning and limiting collected data to 100k observations at most. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "row_count = 1000\n",
    "max_obv = 100\n",
    "\n",
    "#https://pypi.org/project/langdetect/\n",
    "### uncomment this to install, then comment and restart kernel ###\n",
    "# %%capture\n",
    "# !pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "### regex for more lang checking\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IUoJ6TgYPXR"
   },
   "source": [
    "Taking the first 100k observations of a dataset and putting it into an initial DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z8T6rtyMD3n"
   },
   "outputs": [],
   "source": [
    "data_text = []\n",
    "data_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('dataset.csv', chunksize=row_count):\n",
    "  if i < max_obv:\n",
    "    data_text += chunk['text'].tolist()\n",
    "    data_sentiment += chunk['sentiment'].tolist()\n",
    "    i += 1\n",
    "  else:\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSiWZWxsaPgk"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'text': data_text, 'sentiment': data_sentiment})\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvX6BEtraa9i"
   },
   "source": [
    "Function to validate a text body. Valid text will be non-empty strings that are not solely whitespace of at least length 1. Regex and langdetect is used to keep observations that have Latin characters (so that it may be able to filter out text using solely Korean characters for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnV0rg-Xa4KS"
   },
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    if not line:\n",
    "        return np.nan\n",
    "    if line == \"\":\n",
    "        return np.nan\n",
    "    if not bool(line.strip()):\n",
    "        return np.nan\n",
    "    if len(line) < 1:\n",
    "        return np.nan\n",
    "    \n",
    "    if bool(re.match('^(?=.*[a-zA-Z])', line)):\n",
    "        try:\n",
    "            if detect(line) != 'en':\n",
    "                return np.nan\n",
    "        except LangDetectException:\n",
    "            return np.nan\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX9-zm9ja-3b"
   },
   "source": [
    "Function to check if text body is English (detect returns 'en') for the entire dataset. \n",
    "* text_col takes a DataFrame.Series: df['text']\n",
    "* sentiment_col takes a DataFrame.Series: df['sentiment']\n",
    "Returns 3 lists of the same length, truncating the last chunk of observations that are less than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLBYnWoWbwjx"
   },
   "outputs": [],
   "source": [
    "def check_en(text_col, sentiment_col):\n",
    "    en_text = text_col.tolist()\n",
    "    en_sentiment = sentiment_col.tolist()\n",
    "    lang = []\n",
    "    \n",
    "    start = 0\n",
    "    for i in np.arange(row_count, len(en_text), row_count):\n",
    "        #observations <1000 at the end will be lost but impact is negligible\n",
    "        #!!!uncomment print statement below to show progress (recommended)!!!\n",
    "#         print(start, i)\n",
    "        lang += [validate_line(x) for x in en_text[start:i]]\n",
    "        start = i\n",
    "    print(\"Finished English check\")\n",
    "    ### all three return values should be of the same length\n",
    "    return en_text[0:len(lang)], en_sentiment[0:len(lang)], lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjPb4WC_cA3X"
   },
   "source": [
    "Putting the returned lists from check_en into a new DataFrame. df['english'] will be dropped after removing all rows with NaN values (non-English, non valid text bodies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmLd7e1-cWhA"
   },
   "outputs": [],
   "source": [
    "en_data_text, en_data_sentiment, en_data_lang = check_en(df['text'], df['sentiment'])\n",
    "\n",
    "en_df = pd.DataFrame(data={'text': en_data_text, 'sentiment': en_data_sentiment, 'english': en_data_lang})\n",
    "en_df = en_df.dropna()\n",
    "en_df = en_df.drop(columns=['english'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJlVz4iOc56E"
   },
   "source": [
    "Function to change numeric sentiment values into string values, otherwise it will just return the input value (if it was already string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeY9mhQLdELN"
   },
   "outputs": [],
   "source": [
    "def sentiment_to_string(sentiment):\n",
    "    if type(sentiment) == int or type(sentiment) == float:\n",
    "        if sentiment < 0:\n",
    "            return \"negative\"\n",
    "        if sentiment > 0:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKgjImtrdHfZ"
   },
   "source": [
    "Final clean dataset as a DataFrame after this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1byQ-7UzdK6a"
   },
   "outputs": [],
   "source": [
    "en_df['sentiment'] = en_df['sentiment'].apply(sentiment_to_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We have considered 3 different approaches for the proposed solution. For all three approaches, we aim to train different classifiers on a labelled dataset that we mentioned before, and test them on different labelled datasets to achieve the cross-dataset testing as mentioned in the Background section. This way, we can try to find out the generalisability of the models on different datasets and answer our research questions. Also, in each approach, different classifiers are evaluated and tested against our metrics and we will select the best model in each approach. For approaches 1 and 2, we are looking at supervised machine learning methods for the classifiers. And for approach 3, which is beyond the scope of the class (but if we have extra time) we will also do some exploration for some deep learning package’s accuracy on the dataset.\n",
    "<br>\n",
    "\n",
    "**Approach #1 : Supervised ML, using scikit packages to take care of BOTH the data preprocessing and also the NLP feature extraction.**\n",
    "\n",
    "All the preprocessing will be taken care of by the CountVectorizer.\n",
    "Please note that we are following this tutorial from the Scikit documentation <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1). Since we haven’t previously worked with natural language processing problems in class before, we are  following this tutorial’s methods for feature extraction. Additionally, we did a lot of thorough, extensive research on various approaches for extracting features for text input. Some features <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1)<a name=\"featextract\"></a>[<sup>[9]</sup>](#featextract) we would be looking at are bag-of-words feature, tf-idf frequencies <a name=\"wiki\"></a>[<sup>[10]</sup>](#wiki), and word embeddings <a name=\"mediium\"></a>[<sup>[11]</sup>](#medium). \n",
    "\n",
    "Specifically, for the tf-idf frequencies, based on the research we did <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1), <a name=\"wiki\"></a>[<sup>[12]</sup>](#wiki), tf-idf is a great way to see how important each of these words in the text are; tf-idf stands for term frequency-inverse document frequency. For tf-idf, we need to use the \"CountVectorizer\" object from scikit <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1) to get the frequencies and then we can use that for this tf-idf computation. While the tf part is more focused on seeing how frequently each of these words are in our training set, the idf portion of the computation is more focused around information theory aspect and assessing the relevance of the word <a name=\"wiki\"></a>[<sup>[10]</sup>](#wiki).\n",
    "\n",
    "After extracting all the various features, the next step would be to run the various classifiers <a name=\"scikit2\"></a>[<sup>[12]</sup>](#scikit2) on the training dataset. Specifically, these are the following classifiers we are planning on running on these features as discussed in class: Support Vector Classifier, K-nearest neighbors, and Decision trees.\n",
    "Some new classifiers, which we haven’t learned in class yet, which we are also planning to investigate in further detail are neural net, Naive Bayes, Ada Boost, etc. <a name=\"scikit2\"></a>[<sup>[12]</sup>](#scikit2). We will be comparing these classifiers on different datasets and select the best model based on our metrics. \n",
    "\n",
    "In addition, for the training sampling techniques, we will be using the following to do comparisons between the different methods that we learned in class: K-fold validation with training, with training/validation/testing, and Nested cross validation. \n",
    "<br>\n",
    "\n",
    "**Approach #2: Supervised ML, doing data preprocessing and above feature extraction from scratch.**\n",
    "\n",
    "The steps for this approach will be similar to the first approach, however for the data preprocessing we will be doing everything from scratch instead of using CountVectorizer. \n",
    "Specifically, these are some of the data preprocessing techniques that we have researched about <a name=\"empstudy\"></a>[<sup>[13]</sup>](#empstudy)<a name=\"featextractreview\"></a>[<sup>[14]</sup>](#featextractreview) for natural language processing sub-field of machine learning are lemmatization/stemming, tokenization, and looking at part of speech for text.\n",
    "Additionally, we will be looking at more research papers and trying to see more data preprocessing techniques for text input data.\n",
    "We will again compare and contrast each classifier, and select the best model in this approach.\n",
    "<br>\n",
    "\n",
    "**Approach #3 (Above & Beyond, Extra work outside of class algorithms): Running on pre-trained neural networks.**\n",
    "\n",
    "Along with supervised ML approaches, there has been a lot of extensive, thorough research being done in deep learning space for Natural Language Processing. Google’s BERT and Open AI’s GPT-3 <a name=\"top8\"></a>[<sup>[15]</sup>](#top8) are just two of the popular NLP deep learning models out there. \n",
    "We will be loading these pretrained models in the Pytorch library. This is a library we haven’t learned in class, but is widely used in deep learning research problems. Specifically, this is new Machine learning content, in deep learning and transformers space, and we spent time understanding these really cool new ML concepts (links below). \n",
    "After performing the above tasks for the supervised machine learning component of the project, we will also be comparing the accuracy of this model to various pretrained models from HuggingFace for sentiment analysis, and see which model provides best accuracy, specifically BERT pretrained model <a name=\"huggingface1\"></a>[<sup>[16]</sup>](#huggingface1) and Chat GPT-2 pretrained model <a name=\"huggingface2\"></a>[<sup>[17]</sup>](#huggingface2).\n",
    "\n",
    "If time permits, we will also try to train a neural network from scratch for sentiment analysis and do further research on research papers in this space. \n",
    "\n",
    "For all these different classification models, after we selected the best model by observing the results they produce on the labelled datasets, we will test them against an unlabelled dataset and see how accurate they seem on the unlabelled dataset compared to the labelled ones they were trained and tested on. In such a way, we can see not only the cross-dataset testing results on the generalisability of models we selected, and also the predictability of the models when they don't have labelled data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will be using various different error metrics such as precision, recall, and overall accuracy score to evaluate how well those classifiers can generalize in sentiment analysis given different subject matter, and if there is a significant difference in performance between each classifier. Precision and recall will also be used to judge how sensitive our classifiers are to negative and positive sentiments, providing insight as to what pitfalls they are subject to in the human language such as sarcasm.\n",
    "\n",
    "$$Precision = {True Positives \\over Predicted Positives}$$\n",
    "$$Recall = {True Positives \\over Actually Positive}$$\n",
    "$$Overall Accuracy = {True Positives + True Negatives \\over Total Predictions Made}$$\n",
    "\n",
    "We will also use a confusion matrix<a name=\"confusionMatrix\"></a>[<sup>[18]</sup>](#confusionMatrix) to visually represent True Positives, False Negatives, False Positives, and True Negatives, since they are the basis using which precision, recall, and overall accuracy are evaluated. A general Confusion Matrix looks like this:\n",
    "|   | Predicted Label |\n",
    "|---|---|---|\n",
    "|True Label|Positive(PP)|Negative(PN)|\n",
    "|Positive(P)|True Positives|False Negatives|\n",
    "|Negative(N)|False Positives|True Negatives|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the datasets all coming from real people collected through social media, there are ethical concerns over their consent in being part of this project as points of data, even with their words and personal information accessible through the media the data is extracted from (Twitter and Reddit).\n",
    "Proper data cleaning and handling would require giving those people as much anonymity that is possible to be given, such as removing their usernames/handles and even the permalink/id to avoid being traced back to a user or community.\n",
    "\n",
    "The goal of this project is to see if, given the labels already assigned in some datasets, how well can a model be trained on them to label other datasets?\n",
    "As the data comes from a sample of people in the population of those respective media for certain topics, the text content may be biased towards/against certain topics or communities.\n",
    "Some datasets have already included sentiment labels to indicate if they are positive, negative, or neutral.\n",
    "Using this data in training may contribute to the model’s perceived connotation of those and related texts in unlabeled datasets.\n",
    "Conclusions drawn from those results may be harmful to a degree regardless of actual intent or content because of the nature of applying sentiment analysis.\n",
    "This project and those that are working with it do not show any personal support or opposition for any of the data gathered, and to keep the integrity of the project itself, all usable content (those in the English language) can be used without censorship as to not influence the direction of the model toward our personal preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Communication channel is FB Messenger.\n",
    "* Notify of any changes of plans in the communication channel as soon as possible.\n",
    "* Confirm or reschedule meeting times for upcoming week if you know that you will not be able to meet at the specified time. \n",
    "* Come prepared to meetings with completed tasks.\n",
    "* Ask for assistance anytime you feel like you need it.\n",
    "* Don’t be afraid to pair up to learn some new skill together or help out one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date and Time | Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|\n",
    "| 2/22 @2 PM |  Brainstormed topics/questions (all)  | Determined best form of communication; Discussed and decided on final project topic; discussed hypothesis; began background research | \n",
    "| 3/3 @5 PM  | Read reviews of our project  | Tailor our project based on what we learned from peer reviews; discuss further steps   |\n",
    "| 3/8 @8 PM  | Prepare your part for checkpoint submission, Import and wrangle Data, do some EDA | Resolve any issues, submit Checkpoint; Review/edit wrangling; discuss further steps   |\n",
    "| 3/12 @8 PM | Be up to date, bring in any concerns or what still needs to be done. | Review of what is done so far. Discuss further steps |\n",
    "| 3/16 @6 PM | Complete your part(s). Prepare any questions and/or concerns. Last chance to change something| Discuss last steps towards project completion. Distribute work to complete the project |\n",
    "| 3/22 @2 PM | Finish all the gaps, fill in all the nits and bits  | Turn in the Project  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"fakenews\"></a>1.[^](#fakenews): LHoy, N., &amp; Koulouri, T. (2022). Exploring the generalisability of fake news detection models. *2022 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata55660.2022.10020583<br> \n",
    "<a name=\"fakenews2\"></a>2.[^](#fakenews2): Blackledge, C., &amp; Atapour-Abarghouei, A. (2021). Transforming fake news: Robust generalisable news classification using Transformers. *2021 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata52589.2021.9671970<br>\n",
    "<a name=\"hatespeech\"></a>3.[^](#hatespeech): Yin W, Zubiaga A. 2021. Towards generalisable hate speech detection: a review on obstacles and solutions. *PeerJ Computer Science* 7:e598 https://doi.org/10.7717/peerj-cs.598<br>\n",
    "<a name=\"sentian\"></a>4.[^](#sentian): Moore, A., & Rayson, P. (2018). Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for target dependent sentiment analysis. *arXiv preprint arXiv:1806.05219*<br>\n",
    "<a name=\"redditclimate\"></a>5.[^](#redditclimate): https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset<br>\n",
    "<a name=\"twitterredditsentian\"></a>6.[^](#twitterredditsentian): https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset<br>\n",
    "<a name=\"fifatweets\"></a>7.[^](#fifatweets): https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets<br>\n",
    "<a name=\"scikit1\"></a>8.[^](#scikit1): https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#exercise-2-sentiment-analysis-on-movie-reviews<br>\n",
    "<a name=\"featextract\"></a>9.[^](#featextract): Zareapoor, M., &amp; K. R, S. (2015). Feature extraction or feature selection for text classification: A case study on phishing email detection. *International Journal of Information Engineering and Electronic Business*, 7(2), 60–65. https://doi.org/10.5815/ijieeb.2015.02.08<br>\n",
    "<a name=\"wiki\"></a>10.[^](#wiki): https://en.wikipedia.org/wiki/Tf%E2%80%93idf<br>\n",
    "<a name=\"medium\"></a>11.[^](#medium): https://medium.com/geekculture/text-feature-extraction-3-3-word-embeddings-model-e98f3d270dce<br>\n",
    "<a name=\"scikit2\"></a>12.[^](#scikit2): https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html<br>\n",
    "<a name=\"empstudy\"></a>13.[^](#empstudy): Sun, X., Liu, X., Hu, J., &amp; Zhu, J. (2014). Empirical studies on the NLP techniques for source code data preprocessing. *Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies*. https://doi.org/10.1145/2627508.2627514<br>\n",
    "<a name=\"featextractreview\"></a>14.[^](#featextractreview): Asghar, M. Z., Khan, A., Ahmad, S., & Kundi, F. M. (2014). A review of feature extraction in sentiment analysis. *Journal of Basic and Applied Scientific Research*, 4(3), 181-186.<br>\n",
    "<a name=\"top8\"></a>15.[^](#top8): https://analyticsindiamag.com/top-8-pre-trained-nlp-models-developers-must-know/<br>\n",
    "<a name=\"huggingface1\"></a>16.[^](#huggingface1): https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis?text=Yay%21%21<br>\n",
    "<a name=\"huggingface2\"></a>17.[^](#huggingface2): https://huggingface.co/michelecafagna26/gpt2-medium-finetuned-sst2-sentiment?text=yayy%21%21<br>\n",
    "<a name=\"confusionMatrix\"></a>18.[^](#confusionMatrix): https://en.wikipedia.org/wiki/Confusion_matrix<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
