{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Model Performance in Sentiment Analysis\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Deepansha Singh\n",
    "- Raina Song\n",
    "- Ilya Kogan\n",
    "- Winah Ruiz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Natural language processing (NLP) has been making headlines all over the news in recent months due to the release and gaining popularity of ChatGPT (chatbot developed by OpenAI).\n",
    "To take a closer look at the field of NLP, we plan to implement a sentiment analysis, a NLP technique that is used to determine the text attitude on a scale from -1 to 1 where absolute positive input has score 1, absolute negative input has score of -1, and a score of 0 is for neutral sentiment.\n",
    "In this project we examine how general a sentiment analysis tool can be \n",
    "(e.g. when trained on data on a specific topic how well it can perform on other topics that it didn’t encounter before or general topicless data) \n",
    "by using different sampling techniques \n",
    "(k-fold Cross Validation and Nested Cross Validation) \n",
    "with different classifiers \n",
    "(Support Vector Classifier, k-Nearest Neighbors algorithm, Decision Trees, etc.) \n",
    "and quantifying their performance using various error metrics \n",
    "(precision, recall, or accuracy scores).\n",
    "We will be using social media data from Reddit with sentiment labels to train our models, choose the best one according to an error metric, and then test how well our “best” model performs on a new dataset found on Twitter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In previous studies, the generalisability and interoperability of NLP models in tasks such as fake news detection<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews)<a name=\"fakenews2\"></a>[<sup>[2]</sup>](#fakenews2), hate speech detection<a name=\"hatespeech\"></a>[<sup>[3]</sup>](#hatespeech), and sentiment analysis<a name=\"sentian\"></a>[<sup>[4]</sup>](#sentian) were explored. It was found that generalisability is a challenge for NLP algorithms, as they generalize poorly on new and unseen datasets<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews). However, robust NLP models are desired to perform large-scale, cross-categorical classification tasks on the internet or social media platforms. Some studies suggest that the biases found in models may be due to the small datasets the models are trained on, which make the models prone to overfitting issues<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews). Larger and more diverse datasets are required to reduce biases in trained models. It was suggested that cross-dataset testing is a useful tool to evaluate model generalisability performance realistically<a name=\"hatespeech\"></a>[<sup>[3]</sup>](#hatespeech). Due to such reasons, our project is motivated to explore the generalisability of NLP sentiment analysis models on different datasets pertaining to different subject matter.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are performing one of the techniques to quantify text, sentiment analysis, using different classifiers. We are then examining how well our model trained on The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate) can perform on Twitter and Reddit Sentimental Analysis Dataset<a name=\"twitterredditsentian\"></a>[<sup>[6]</sup>](#twitterredditsentian) and FIFA World Cup 2022 Tweets<a name=\"fifatweets\"></a>[<sup>[7]</sup>](#fifatweets) by using different error metrics (precision, recall, or accuracy scores) and how does a choice of error metric change what is the best model (i.e. whether we have a completely best model or not). One potential solution to our problem is to use CountVectorizer to do the preprocessing, then extract features using tf-idf frequincies approach after which fit some classifiers (e.g. Support Vector Classifier, k-NN, Decision Trees, and AdaBoost) on The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate). Then, use an error metric, for example, precision or recall and a confusion matrix for visualizations to determine which Classifier gives the best performance on different datasets using, for example, k-fold Cross Validation or Nested Cross Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsyUP33PKbtB"
   },
   "source": [
    "### (pavellexyr) The Reddit Climate Change Dataset: ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
    "\n",
    "There are two .csv files, one containing comments from Reddit on climate change and the other containing posts on climate change. For this project, we will only be using the .csv file with comments as there is already sentiment analysis done on the 4.6 million observations collected.\n",
    "\n",
    "An observation in this dataset consists of:\n",
    "* Type of datapoint (comment)\n",
    "* Unique ID of the comment\n",
    "* Unique ID of the comment’s subreddit\n",
    "* The name of the subreddit the comment was found on\n",
    "* If the comment’s subreddit is NSFW\n",
    "* The timestamp (UTC) of the comment\n",
    "* Permalink to the comment\n",
    "* Body text of the comment\n",
    "* Analyzed sentiment for the comment as a continuous value from [-1, 1]\n",
    "* Comment’s score (votes on Reddit)\n",
    "\n",
    "### (cosmos98) Twitter and Reddit Sentimental analysis Dataset: ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n",
    "\n",
    "There are two .csv files, one containing comments from Reddit (36k observations) and the other containing tweets from Twitter (162k observations). The Twitter dataset was extracted with the focus on tweets people made about the Indian Prime Minister Narendra Modi. The Reddit dataset has no indication of having a specific area or topic they were sourced from. \n",
    "\n",
    "Both datasets only have two variables:\n",
    "* The text of the comment or tweet\n",
    "* The category/sentiment of the text {-1, 0, 1}\n",
    "\n",
    "### (tirendazacademy) FIFA World Cup 2022 Tweets:  ###\n",
    "Retrieved from: https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets\n",
    "\n",
    "There is one .csv file containing tweets regarding the 2022 FIFA World Cup, a dataset of about 22k observations.\n",
    "\n",
    "An observation in this dataset contains:\n",
    "* ID (index) of the observation\n",
    "* Date the tweet was created\n",
    "* Number of likes the tweet had\n",
    "* Source of the tweet (Twitter of iPhone, Twitter for Android)\n",
    "* The body text of the tweet\n",
    "* Sentiment of the tweet as strings: “positive”, “neutral”, or “negative”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3CQ_PPVLPmQ"
   },
   "source": [
    "For all the datasets discussed above, there are only two variables we are concerned with: the text of the comment/tweet and the sentiment the text was already given. Due to some datasets being incredibly large, only the first 100k observations of each dataset will be used. \n",
    "\n",
    "All datasets will undergo further data cleaning. To the best of our ability, we will filter our dataset to only have English detected text using the langdetect library and regex. Other unusable observations (such as rows containing NaN values) will also be excluded. This results in slightly less than the upper limit of 100k observations initially taken from each dataset. In addition, the existing numeric labels for sentiment some datasets may have will be changed to string values of “positive”, “negative”, and “neutral” to be consistent with each other and only have 3 total classes for classification. \n",
    "\n",
    "The full code for cleaning the files used up to this point are in the “COGS118A replacement data cleaning.ipynb” file in this repository. Below will be code snippets, mainly of the functions created to clean the data. For demonstration purposes, these will just be example variable names instead of what was actually used in practice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OuOw8mHvLWCP"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Libraries and global variables to be used for cleaning and limiting collected data to 100k observations at most. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "row_count = 1000\n",
    "max_obv = 100\n",
    "\n",
    "#https://pypi.org/project/langdetect/\n",
    "### uncomment this to install, then comment and restart kernel ###\n",
    "# %%capture\n",
    "# !pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "### regex for more lang checking\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IUoJ6TgYPXR"
   },
   "source": [
    "Taking the first 100k observations of a dataset and putting it into an initial DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z8T6rtyMD3n"
   },
   "outputs": [],
   "source": [
    "data_text = []\n",
    "data_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('dataset.csv', chunksize=row_count):\n",
    "  if i < max_obv:\n",
    "    data_text += chunk['text'].tolist()\n",
    "    data_sentiment += chunk['sentiment'].tolist()\n",
    "    i += 1\n",
    "  else:\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSiWZWxsaPgk"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'text': data_text, 'sentiment': data_sentiment})\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvX6BEtraa9i"
   },
   "source": [
    "Function to validate a text body. Valid text will be non-empty strings that are not solely whitespace of at least length 1. Regex and langdetect is used to keep observations that have Latin characters (so that it may be able to filter out text using solely Korean characters for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnV0rg-Xa4KS"
   },
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    if not line:\n",
    "        return np.nan\n",
    "    if line == \"\":\n",
    "        return np.nan\n",
    "    if not bool(line.strip()):\n",
    "        return np.nan\n",
    "    if len(line) < 1:\n",
    "        return np.nan\n",
    "    \n",
    "    if bool(re.match('^(?=.*[a-zA-Z])', line)):\n",
    "        try:\n",
    "            if detect(line) != 'en':\n",
    "                return np.nan\n",
    "        except LangDetectException:\n",
    "            return np.nan\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX9-zm9ja-3b"
   },
   "source": [
    "Function to check if text body is English (detect returns 'en') for the entire dataset. \n",
    "* text_col takes a DataFrame.Series: df['text']\n",
    "* sentiment_col takes a DataFrame.Series: df['sentiment']\n",
    "Returns 3 lists of the same length, truncating the last chunk of observations that are less than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLBYnWoWbwjx"
   },
   "outputs": [],
   "source": [
    "def check_en(text_col, sentiment_col):\n",
    "    en_text = text_col.tolist()\n",
    "    en_sentiment = sentiment_col.tolist()\n",
    "    lang = []\n",
    "    \n",
    "    start = 0\n",
    "    for i in np.arange(row_count, len(en_text), row_count):\n",
    "        #observations <1000 at the end will be lost but impact is negligible\n",
    "        #!!!uncomment print statement below to show progress (recommended)!!!\n",
    "#         print(start, i)\n",
    "        lang += [validate_line(x) for x in en_text[start:i]]\n",
    "        start = i\n",
    "    print(\"Finished English check\")\n",
    "    ### all three return values should be of the same length\n",
    "    return en_text[0:len(lang)], en_sentiment[0:len(lang)], lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjPb4WC_cA3X"
   },
   "source": [
    "Putting the returned lists from check_en into a new DataFrame. df['english'] will be dropped after removing all rows with NaN values (non-English, non valid text bodies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmLd7e1-cWhA"
   },
   "outputs": [],
   "source": [
    "en_data_text, en_data_sentiment, en_data_lang = check_en(df['text'], df['sentiment'])\n",
    "\n",
    "en_df = pd.DataFrame(data={'text': en_data_text, 'sentiment': en_data_sentiment, 'english': en_data_lang})\n",
    "en_df = en_df.dropna()\n",
    "en_df = en_df.drop(columns=['english'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJlVz4iOc56E"
   },
   "source": [
    "Function to change numeric sentiment values into string values, otherwise it will just return the input value (if it was already string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeY9mhQLdELN"
   },
   "outputs": [],
   "source": [
    "def sentiment_to_string(sentiment):\n",
    "    if type(sentiment) == int or type(sentiment) == float:\n",
    "        if sentiment < 0:\n",
    "            return \"negative\"\n",
    "        if sentiment > 0:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKgjImtrdHfZ"
   },
   "source": [
    "Final clean dataset as a DataFrame after this line. The cleaned files with 'en_' at the beginning are found here as .csv files: https://drive.google.com/drive/folders/1fw7OgFNacjARe3FF_FpDBAGEsDAIlqZJ?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1byQ-7UzdK6a"
   },
   "outputs": [],
   "source": [
    "en_df['sentiment'] = en_df['sentiment'].apply(sentiment_to_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We have considered 3 different approaches for the proposed solution. For all three approaches, we aim to train different classifiers on a labelled dataset that we mentioned before, and test them on different labelled datasets to achieve the cross-dataset testing as mentioned in the Background section. This way, we can try to find out the generalisability of the models on different datasets and answer our research question. Also, in each approach, different classifiers are evaluated and tested against our metrics and we will select the best model in each approach. For approaches 1 and 2, we are looking at supervised machine learning methods for the classifiers. And for approach 3, which is beyond the scope of the class (but if we have extra time) we will also do some exploration for some deep learning package’s accuracy on the dataset.\n",
    "<br>\n",
    "\n",
    "**Approach #1 : Supervised ML, using scikit packages to take care of BOTH the data preprocessing and also the NLP feature extraction.**\n",
    "\n",
    "All the preprocessing will be taken care of by the CountVectorizer.\n",
    "Please note that we are following this tutorial from the Scikit documentation <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1). Since we haven’t previously worked with natural language processing problems in class before, we are  following this tutorial’s methods for feature extraction. Additionally, we did a lot of thorough, extensive research on various approaches for extracting features for text input. Some features <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1)<a name=\"featextract\"></a>[<sup>[9]</sup>](#featextract) we would be looking at are bag-of-words feature, tf-idf frequencies <a name=\"wiki\"></a>[<sup>[10]</sup>](#wiki), and word embeddings <a name=\"mediium\"></a>[<sup>[11]</sup>](#medium). \n",
    "\n",
    "Specifically, for the tf-idf frequencies, based on the research we did <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1), <a name=\"wiki\"></a>[<sup>[12]</sup>](#wiki), tf-idf is a great way to see how important each of these words in the text are; tf-idf stands for term frequency-inverse document frequency. For tf-idf, we need to use the \"CountVectorizer\" object from scikit <a name=\"scikit1\"></a>[<sup>[8]</sup>](#scikit1) to get the frequencies and then we can use that for this tf-idf computation. While the tf part is more focused on seeing how frequently each of these words are in our training set, the idf portion of the computation is more focused around information theory aspect and assessing the relevance of the word <a name=\"wiki\"></a>[<sup>[10]</sup>](#wiki).\n",
    "\n",
    "After extracting all the various features, the next step would be to run the various classifiers <a name=\"scikit2\"></a>[<sup>[12]</sup>](#scikit2) on the training dataset. Specifically, these are the following classifiers we are planning on running on these features as discussed in class: Support Vector Classifier, K-nearest neighbors, and Decision trees.\n",
    "Some new classifiers, which we haven’t learned in class yet, which we are also planning to investigate in further detail are neural net, Naive Bayes, Ada Boost, etc. <a name=\"scikit2\"></a>[<sup>[12]</sup>](#scikit2). We will be comparing these classifiers on different datasets and select the best model based on our metrics. \n",
    "\n",
    "In addition, for the training sampling techniques, we will be using the following to do comparisons between the different methods that we learned in class: K-fold validation with training, with training/validation/testing, and Nested cross validation. \n",
    "<br>\n",
    "\n",
    "**Approach #2: Supervised ML, doing data preprocessing and above feature extraction from scratch.**\n",
    "\n",
    "The steps for this approach will be similar to the first approach, however for the data preprocessing we will be doing everything from scratch instead of using CountVectorizer. \n",
    "Specifically, these are some of the data preprocessing techniques that we have researched about <a name=\"empstudy\"></a>[<sup>[13]</sup>](#empstudy)<a name=\"featextractreview\"></a>[<sup>[14]</sup>](#featextractreview) for natural language processing sub-field of machine learning are lemmatization/stemming, tokenization, and looking at part of speech for text.\n",
    "Additionally, we will be looking at more research papers and trying to see more data preprocessing techniques for text input data.\n",
    "We will again compare and contrast each classifier, and select the best model in this approach.\n",
    "<br>\n",
    "\n",
    "**Approach #3 (Above & Beyond, Extra work outside of class algorithms): Running on pre-trained neural networks.**\n",
    "\n",
    "Along with supervised ML approaches, there has been a lot of extensive, thorough research being done in deep learning space for Natural Language Processing. Google’s BERT and Open AI’s GPT-3 <a name=\"top8\"></a>[<sup>[15]</sup>](#top8) are just two of the popular NLP deep learning models out there. \n",
    "We will be loading these pretrained models in the Pytorch library. This is a library we haven’t learned in class, but is widely used in deep learning research problems. Specifically, this is new Machine learning content, in deep learning and transformers space, and we spent time understanding these really cool new ML concepts (links below). \n",
    "After performing the above tasks for the supervised machine learning component of the project, we will also be comparing the accuracy of this model to various pretrained models from HuggingFace for sentiment analysis, and see which model provides best accuracy, specifically BERT pretrained model <a name=\"huggingface1\"></a>[<sup>[16]</sup>](#huggingface1) and Chat GPT-2 pretrained model <a name=\"huggingface2\"></a>[<sup>[17]</sup>](#huggingface2).\n",
    "\n",
    "If time permits, we will also try to train a neural network from scratch for sentiment analysis and do further research on research papers in this space. \n",
    "\n",
    "For all these different classification models, after we selected the best model by observing the results they produce on the labelled datasets, we will test them against an unlabelled dataset and see how accurate they seem on the unlabelled dataset compared to the labelled ones they were trained and tested on. In such a way, we can see not only the cross-dataset testing results on the generalisability of models we selected, and also the predictability of the models when they don't have labelled data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will be using various different error metrics such as precision, recall, and overall accuracy score to evaluate how well those classifiers can generalize in sentiment analysis given different subject matter, and if there is a significant difference in performance between each classifier. Precision and recall will also be used to judge how sensitive our classifiers are to negative and positive sentiments, providing insight as to what pitfalls they are subject to in the human language such as sarcasm.\n",
    "\n",
    "$$Precision = {True Positives \\over Predicted Positives}$$\n",
    "$$Recall = {True Positives \\over Actually Positive}$$\n",
    "$$Overall Accuracy = {True Positives + True Negatives \\over Total Predictions Made}$$\n",
    "\n",
    "We will also use a confusion matrix<a name=\"confusionMatrix\"></a>[<sup>[18]</sup>](#confusionMatrix) to visually represent True Positives, False Negatives, False Positives, and True Negatives, since they are the basis using which precision, recall, and overall accuracy are evaluated. A general Confusion Matrix looks like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    |Predicted Positive(PP)| Predicted Negative(PN)|\n",
    "| ------------------ | -------------------- | --------------------- |\n",
    "| Actual Positive(P) |    True Positives    |    False Negatives    |\n",
    "| Actual Negative(N) |    False Positives   |    True Negatives     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"fakenews\"></a>1.[^](#fakenews): LHoy, N., &amp; Koulouri, T. (2022). Exploring the generalisability of fake news detection models. *2022 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata55660.2022.10020583<br> \n",
    "<a name=\"fakenews2\"></a>2.[^](#fakenews2): Blackledge, C., &amp; Atapour-Abarghouei, A. (2021). Transforming fake news: Robust generalisable news classification using Transformers. *2021 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata52589.2021.9671970<br>\n",
    "<a name=\"hatespeech\"></a>3.[^](#hatespeech): Yin W, Zubiaga A. 2021. Towards generalisable hate speech detection: a review on obstacles and solutions. *PeerJ Computer Science* 7:e598 https://doi.org/10.7717/peerj-cs.598<br>\n",
    "<a name=\"sentian\"></a>4.[^](#sentian): Moore, A., & Rayson, P. (2018). Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for target dependent sentiment analysis. *arXiv preprint arXiv:1806.05219*<br>\n",
    "<a name=\"redditclimate\"></a>5.[^](#redditclimate): https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset<br>\n",
    "<a name=\"twitterredditsentian\"></a>6.[^](#twitterredditsentian): https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset<br>\n",
    "<a name=\"fifatweets\"></a>7.[^](#fifatweets): https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets<br>\n",
    "<a name=\"scikit1\"></a>8.[^](#scikit1): https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#exercise-2-sentiment-analysis-on-movie-reviews<br>\n",
    "<a name=\"featextract\"></a>9.[^](#featextract): Zareapoor, M., &amp; K. R, S. (2015). Feature extraction or feature selection for text classification: A case study on phishing email detection. *International Journal of Information Engineering and Electronic Business*, 7(2), 60–65. https://doi.org/10.5815/ijieeb.2015.02.08<br>\n",
    "<a name=\"wiki\"></a>10.[^](#wiki): https://en.wikipedia.org/wiki/Tf%E2%80%93idf<br>\n",
    "<a name=\"medium\"></a>11.[^](#medium): https://medium.com/geekculture/text-feature-extraction-3-3-word-embeddings-model-e98f3d270dce<br>\n",
    "<a name=\"scikit2\"></a>12.[^](#scikit2): https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html<br>\n",
    "<a name=\"empstudy\"></a>13.[^](#empstudy): Sun, X., Liu, X., Hu, J., &amp; Zhu, J. (2014). Empirical studies on the NLP techniques for source code data preprocessing. *Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies*. https://doi.org/10.1145/2627508.2627514<br>\n",
    "<a name=\"featextractreview\"></a>14.[^](#featextractreview): Asghar, M. Z., Khan, A., Ahmad, S., & Kundi, F. M. (2014). A review of feature extraction in sentiment analysis. *Journal of Basic and Applied Scientific Research*, 4(3), 181-186.<br>\n",
    "<a name=\"top8\"></a>15.[^](#top8): https://analyticsindiamag.com/top-8-pre-trained-nlp-models-developers-must-know/<br>\n",
    "<a name=\"huggingface1\"></a>16.[^](#huggingface1): https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis?text=Yay%21%21<br>\n",
    "<a name=\"huggingface2\"></a>17.[^](#huggingface2): https://huggingface.co/michelecafagna26/gpt2-medium-finetuned-sst2-sentiment?text=yayy%21%21<br>\n",
    "<a name=\"confusionMatrix\"></a>18.[^](#confusionMatrix): https://en.wikipedia.org/wiki/Confusion_matrix<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
