{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63260db6",
   "metadata": {},
   "source": [
    "### Note on large .csv files ###\n",
    "The Reddit Climate Change Dataset (pavellexyr) and Sentiment140 (kazanova) are huge files with over 1 million observations. I cannot upload them to GitHub so here are the links to each one from Kaggle. There is also one dataset that is mentioned but has no associated code: Twitter US Airline Sentiment (crowdflower). This is included below if, for any reason, it is needed in the future.\n",
    "\n",
    "The Reddit Climate Change Dataset (pavellexyr): https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
    "\n",
    "Sentiment140 dataset with 1.6 million tweets (kazanova): https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "Twitter US Airline Sentiment (crowdflower): https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f65a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72d0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not having the limit breaks the kernel. This takes only the first 100K observations per dataset\n",
    "row_count = 1000\n",
    "max_obv = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47d4684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/rainasong/Library/Python/3.11/lib/python/site-packages (from langdetect) (1.16.0)\n",
      "Installing collected packages: langdetect\n",
      "\u001b[33m  DEPRECATION: langdetect is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for langdetect ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed langdetect-1.0.9\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450aa706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/langdetect/\n",
    "\n",
    "### uncomment this to install, then comment and restart kernel ###\n",
    "# %%capture\n",
    "# !pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "### regex for more lang checking\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fe043",
   "metadata": {},
   "source": [
    "### (pavellexyr) The Reddit Climate Change Dataset ###\n",
    "Only using the-reddit-climate-change-dataset-comments.csv which has a column for sentiment for the text. This file alone is over 4GB and processing all of it breaks the kernel so only the first 100K are processed. The columns of interest are 'body' for the comment's body text, and 'sentiment' for the analyzed sentiment per text. NaN values are dropped, leaving a little under 100K usuable observations. \n",
    "\n",
    "This is the only dataset that has continuous sentiment values [-1, 1] instead of discrete {-1, 0, 1} so if using ALL of these datasets together as one, it may be biased toward those discrete values than anything in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fa0d0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rclimate_text = []\n",
    "rclimate_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('the-reddit-climate-change-dataset-comments.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        rclimate_text += chunk['body'].tolist()\n",
    "        rclimate_sentiment += chunk['sentiment'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fc3915c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rclimate_df = pd.DataFrame(data={'text': rclimate_text, 'sentiment': rclimate_sentiment})\n",
    "rclimate_df = rclimate_df.dropna()\n",
    "# rclimate_df.shape #expected (98422, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb035204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    if not line:\n",
    "        return np.nan\n",
    "    if line == \"\":\n",
    "        return np.nan\n",
    "    if not bool(line.strip()):\n",
    "        return np.nan\n",
    "    if len(line) < 1:\n",
    "        return np.nan\n",
    "    \n",
    "    if bool(re.match('^(?=.*[a-zA-Z])', line)):\n",
    "        try:\n",
    "            if detect(line) != 'en':\n",
    "                return np.nan\n",
    "        except LangDetectException:\n",
    "            return np.nan\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1bd7bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### expect 10 minutes to run for 100k rows\n",
    "### text_col is df['text']\n",
    "### sentiment_col is df['sentiment']\n",
    "### returns three lists of the same length\n",
    "\n",
    "def check_en(text_col, sentiment_col):\n",
    "    en_text = text_col.tolist()\n",
    "    en_sentiment = sentiment_col.tolist()\n",
    "    lang = []\n",
    "    \n",
    "    start = 0\n",
    "    for i in np.arange(row_count, len(en_text), row_count):\n",
    "        #observations <1000 at the end will be lost but impact is negligible\n",
    "        #!!!uncomment print statement below to show progress (recommended)!!!\n",
    "#         print(start, i)\n",
    "        lang += [validate_line(x) for x in en_text[start:i]]\n",
    "        start = i\n",
    "    print(\"Finished English check\")\n",
    "    ### all three return values should be of the same length\n",
    "    return en_text[0:len(lang)], en_sentiment[0:len(lang)], lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23cf4d21",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m en_climate_text, en_climate_sentiment, en_climate_lang \u001b[39m=\u001b[39m check_en(rclimate_df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m], rclimate_df[\u001b[39m'\u001b[39;49m\u001b[39msentiment\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      2\u001b[0m \u001b[39m#all should be len 98000\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m, in \u001b[0;36mcheck_en\u001b[0;34m(text_col, sentiment_col)\u001b[0m\n\u001b[1;32m     11\u001b[0m     start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(row_count, \u001b[39mlen\u001b[39m(en_text), row_count):\n\u001b[1;32m     13\u001b[0m         \u001b[39m#observations <1000 at the end will be lost but impact is negligible\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[39m#!!!uncomment print statement below to show progress (recommended)!!!\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m#         print(start, i)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         lang \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [validate_line(x) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m en_text[start:i]]\n\u001b[1;32m     17\u001b[0m         start \u001b[39m=\u001b[39m i\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished English check\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     start \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(row_count, \u001b[39mlen\u001b[39m(en_text), row_count):\n\u001b[1;32m     13\u001b[0m         \u001b[39m#observations <1000 at the end will be lost but impact is negligible\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[39m#!!!uncomment print statement below to show progress (recommended)!!!\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m#         print(start, i)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         lang \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [validate_line(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m en_text[start:i]]\n\u001b[1;32m     17\u001b[0m         start \u001b[39m=\u001b[39m i\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished English check\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[44], line 13\u001b[0m, in \u001b[0;36mvalidate_line\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mbool\u001b[39m(re\u001b[39m.\u001b[39mmatch(\u001b[39m'\u001b[39m\u001b[39m^(?=.*[a-zA-Z])\u001b[39m\u001b[39m'\u001b[39m, line)):\n\u001b[1;32m     12\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m         \u001b[39mif\u001b[39;00m detect(line) \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     14\u001b[0m             \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mnan\n\u001b[1;32m     15\u001b[0m     \u001b[39mexcept\u001b[39;00m LangDetectException:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector_factory.py:130\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    128\u001b[0m detector \u001b[39m=\u001b[39m _factory\u001b[39m.\u001b[39mcreate()\n\u001b[1;32m    129\u001b[0m detector\u001b[39m.\u001b[39mappend(text)\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m detector\u001b[39m.\u001b[39;49mdetect()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:136\u001b[0m, in \u001b[0;36mDetector.detect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    133\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Detect language of the target text and return the language name\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    which has the highest probability.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     probabilities \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_probabilities()\n\u001b[1;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m probabilities:\n\u001b[1;32m    138\u001b[0m         \u001b[39mreturn\u001b[39;00m probabilities[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlang\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:143\u001b[0m, in \u001b[0;36mDetector.get_probabilities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_probabilities\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlangprob \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_detect_block()\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_probability(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlangprob)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:161\u001b[0m, in \u001b[0;36mDetector._detect_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_lang_prob(prob, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(ngrams), alpha)\n\u001b[1;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_prob(prob) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONV_THRESHOLD \u001b[39mor\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mITERATION_LIMIT:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langdetect/detector.py:212\u001b[0m, in \u001b[0;36mDetector._update_lang_prob\u001b[0;34m(self, prob, word, alpha)\u001b[0m\n\u001b[1;32m    210\u001b[0m weight \u001b[39m=\u001b[39m alpha \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBASE_FREQ\n\u001b[1;32m    211\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m xrange(\u001b[39mlen\u001b[39m(prob)):\n\u001b[0;32m--> 212\u001b[0m     prob[i] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m weight \u001b[39m+\u001b[39m lang_prob_map[i]\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "en_climate_text, en_climate_sentiment, en_climate_lang = check_en(rclimate_df['text'], rclimate_df['sentiment'])\n",
    "#all should be len 98000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_climate_df = pd.DataFrame(data={'text': en_climate_text, 'sentiment': en_climate_sentiment, 'english': en_climate_lang})\n",
    "en_climate_df = en_climate_df.dropna()\n",
    "en_climate_df = en_climate_df.drop(columns=['english'])\n",
    "# print(en_climate_df.shape) #expected (97759, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee815f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_string(sentiment):\n",
    "    if type(sentiment) == int or type(sentiment) == float:\n",
    "        if sentiment < 0:\n",
    "            return \"negative\"\n",
    "        if sentiment > 0:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_climate_df['sentiment'] = en_climate_df['sentiment'].apply(sentiment_to_string)\n",
    "# en_climate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f0fd0",
   "metadata": {},
   "source": [
    "### (cosmos98) Twitter and Reddit Sentimental analysis Dataset ###\n",
    "Like with the dataset above, observations are limited to the first 100K and reduced to not have NaN values. Numeric values of {-1, 0, 1} are changed to {negative, neutral, positive} respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0adf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_twitter_text = []\n",
    "cosmos_twitter_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('Twitter_Data.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        cosmos_twitter_text += chunk['clean_text'].tolist()\n",
    "        cosmos_twitter_sentiment += chunk['category'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cosmos_reddit_text = []\n",
    "cosmos_reddit_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('Reddit_Data.csv', chunksize=row_count):\n",
    "    if i <max_obv:\n",
    "        cosmos_reddit_text += chunk['clean_comment'].tolist()\n",
    "        cosmos_reddit_sentiment += chunk['category'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1c3138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosmos_twitter_df = pd.DataFrame(data={'text': cosmos_twitter_text, 'sentiment': cosmos_twitter_sentiment}).dropna()\n",
    "cosmos_reddit_df = pd.DataFrame(data={'text': cosmos_reddit_text, 'sentiment': cosmos_reddit_sentiment}).dropna()\n",
    "# cosmos_twitter_df.shape #expected (99999, 2)\n",
    "# cosmos_reddit_df.shape #expected (37149, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b645b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised ‚Äúminimum government maximum...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>only modi theres question compare freebies wit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>modi disgrace always people who have expectati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>why want vote for modi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>why didnt people say this intolerance all beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>modi body language has change lot seems like f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0      when modi promised ‚Äúminimum government maximum...         -1\n",
       "1      talk all the nonsense and continue all the dra...          0\n",
       "2      what did just say vote for modi  welcome bjp t...          1\n",
       "3      asking his supporters prefix chowkidar their n...          1\n",
       "4      answer who among these the most powerful world...          1\n",
       "...                                                  ...        ...\n",
       "99995  only modi theres question compare freebies wit...          0\n",
       "99996  modi disgrace always people who have expectati...          1\n",
       "99997                           why want vote for modi            0\n",
       "99998  why didnt people say this intolerance all beca...          0\n",
       "99999  modi body language has change lot seems like f...          0\n",
       "\n",
       "[99999 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmos_twitter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f40db3",
   "metadata": {},
   "source": [
    "### (cosmos98) Twitter dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7c6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished English check\n"
     ]
    }
   ],
   "source": [
    "en_cosmos_twitter_text, en_cosmos_twitter_sentiment, en_cosmos_twitter_lang = check_en(cosmos_twitter_df['text'], cosmos_twitter_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c14db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91822, 2)\n"
     ]
    }
   ],
   "source": [
    "en_cosmos_twitter_df = pd.DataFrame(data={'text': en_cosmos_twitter_text, 'sentiment':en_cosmos_twitter_sentiment, 'english':en_cosmos_twitter_lang})\n",
    "en_cosmos_twitter_df = en_cosmos_twitter_df.dropna()\n",
    "en_cosmos_twitter_df = en_cosmos_twitter_df.drop(columns=['english'])\n",
    "print(en_cosmos_twitter_df.shape) #expected (91952, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a548ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised ‚Äúminimum government maximum...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98995</th>\n",
       "      <td>india cant survive another term modi</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98996</th>\n",
       "      <td>modi hands down indians are too influenced bol...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98997</th>\n",
       "      <td>rajdeep known congress supporters from day one...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98998</th>\n",
       "      <td>its bcoz they hate modi thats</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98999</th>\n",
       "      <td>narendra modi begin campaign trail address pub...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91822 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      when modi promised ‚Äúminimum government maximum...  negative\n",
       "1      talk all the nonsense and continue all the dra...   neutral\n",
       "2      what did just say vote for modi  welcome bjp t...  positive\n",
       "3      asking his supporters prefix chowkidar their n...  positive\n",
       "4      answer who among these the most powerful world...  positive\n",
       "...                                                  ...       ...\n",
       "98995               india cant survive another term modi   neutral\n",
       "98996  modi hands down indians are too influenced bol...  negative\n",
       "98997  rajdeep known congress supporters from day one...   neutral\n",
       "98998                     its bcoz they hate modi thats   negative\n",
       "98999  narendra modi begin campaign trail address pub...   neutral\n",
       "\n",
       "[91822 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_cosmos_twitter_df['sentiment'] = en_cosmos_twitter_df['sentiment'].apply(sentiment_to_string)\n",
    "# en_cosmos_twitter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46bf5d2",
   "metadata": {},
   "source": [
    "### (cosmos98) Reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7439005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished English check\n"
     ]
    }
   ],
   "source": [
    "en_cosmos_reddit_text, en_cosmos_reddit_sentiment, en_cosmos_reddit_lang = check_en(cosmos_reddit_df['text'], cosmos_reddit_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d02c0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_cosmos_reddit_df = pd.DataFrame(data={'text': en_cosmos_reddit_text, 'sentiment': en_cosmos_reddit_sentiment, 'english':en_cosmos_reddit_lang})\n",
    "en_cosmos_reddit_df = en_cosmos_reddit_df.dropna()\n",
    "en_cosmos_reddit_df = en_cosmos_reddit_df.drop(columns=['english'])\n",
    "# print(en_cosmos_reddit_df.shape) #expected (31669, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e6016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_cosmos_reddit_df['sentiment'] = en_cosmos_reddit_df['sentiment'].apply(sentiment_to_string)\n",
    "# en_cosmos_reddit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b55ac1",
   "metadata": {},
   "source": [
    "### NOT USABLE -- (kazanova) Sentiment140 dataset with 1.6 million tweets -- NOT USABLE ###\n",
    "Dataset originally has 1.6 million tweets, limited to 100K. Dropped NaN values\n",
    "\n",
    "Unlike the previous datasets, sentiment is recorded as 0=negative, 2=neutral, 4=positive.\n",
    "However, the target column that is supposed to record this is entirely 0 which is unlikely for a set of 1.6 million tweets. Therefore, the sentiment from this dataset cannot be used with the others. It might be saved for a seperate purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e54983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kazanova_text = []\n",
    "# kazanova_sentiment = []\n",
    "# #this dataset used the first observation as the columns\n",
    "# col_text = 5\n",
    "# col_sentiment = 0\n",
    "# i = 0\n",
    "# for chunk in pd.read_csv('118Adatasets/kazanova_sentiment140.csv', chunksize=row_count):\n",
    "#     if i < max_obv:\n",
    "#         kazanova_text += chunk.iloc[:,5].tolist()\n",
    "#         kazanova_sentiment += chunk.iloc[:,0].tolist()\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec0168a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kazanova_df = pd.DataFrame(data={'text':kazanova_text, 'sentiment':kazanova_sentiment})\n",
    "# kazanova_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd37f03",
   "metadata": {},
   "source": [
    "### NOT USABLE -- (crowdflower) Twitter US Airline Sentiment -- NOT USABLE ###\n",
    "Dataset has Tweet id for the text, but not the actual text itself. Sentiment is recorded in strings as 'positive', 'negative' and 'neutral' which can be converted to numerical values of 1, -1, and 0 respectively. There is no text readily available in the file so I will not write code for the numeric conversion here. Unlike the kazanova dataset, we likely cannot use this for another purpose without taking significant time to extract the text from the Tweet id for each observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbec67",
   "metadata": {},
   "source": [
    "### (tirendazacademy) FIFA World Cup 2022 Tweets ###\n",
    "Has Tweet text body and sentiment in strings of 'positive', 'negative' and 'neutral' which are unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3e585da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_text = []\n",
    "fifa_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('fifa_world_cup_2022_tweets.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        fifa_text += chunk['Tweet'].tolist()\n",
    "        fifa_sentiment += chunk['Sentiment'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a743e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_df = pd.DataFrame(data={'text':fifa_text,'sentiment':fifa_sentiment}).dropna()\n",
    "# fifa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "baef7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished English check\n"
     ]
    }
   ],
   "source": [
    "en_fifa_text, en_fifa_sentiment, en_fifa_lang = check_en(fifa_df['text'], fifa_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "399f35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fifa_df = pd.DataFrame(data={'text': en_fifa_text, 'sentiment': en_fifa_sentiment, 'english': en_fifa_lang})\n",
    "en_fifa_df = en_fifa_df.dropna()\n",
    "en_fifa_df = en_fifa_df.drop(columns=['english'])\n",
    "# print(en_fifa_df.shape) #expected (21798, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d2860",
   "metadata": {},
   "source": [
    "Below are the cleaned DataFrames, having only the columns 'text' (unchanged) and 'sentiment' which has values \"positive\", \"negative\" or \"neutral\". The maximum number of observations per dataset is 100k rows, but any NaN values and text bodies that were not detected to be English (truncated to the nearest thousand) are dropped to make the data more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11f48a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_climate_df #shape (97759, 2)\n",
    "# en_cosmos_reddit_df #shape (31669, 2)\n",
    "# en_cosmos_twitter_df #shape (91952, 2)\n",
    "# en_fifa_df #shape (21798, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51066d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Yeah but what the above commenter is saying is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Any comparison of efficiency between solar and...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm honestly waiting for climate change and th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Not just Sacramento. It's actually happening a...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I think climate change tends to get some peopl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97744</th>\n",
       "      <td>97995</td>\n",
       "      <td>It‚Äôs almost like climate change is real.   Huh.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97745</th>\n",
       "      <td>97996</td>\n",
       "      <td>I'm not sure I agree. More Americans are consi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97746</th>\n",
       "      <td>97997</td>\n",
       "      <td>If 40 billion could fix climate change why did...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97747</th>\n",
       "      <td>97998</td>\n",
       "      <td>There are a lot of answers to climate change, ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97748</th>\n",
       "      <td>97999</td>\n",
       "      <td>He‚Äôs full of shit. It‚Äôs outlined and debunked ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97749 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text sentiment\n",
       "0               0  Yeah but what the above commenter is saying is...  positive\n",
       "1               1  Any comparison of efficiency between solar and...  negative\n",
       "2               2  I'm honestly waiting for climate change and th...  negative\n",
       "3               3  Not just Sacramento. It's actually happening a...   neutral\n",
       "4               4  I think climate change tends to get some peopl...  positive\n",
       "...           ...                                                ...       ...\n",
       "97744       97995    It‚Äôs almost like climate change is real.   Huh.  positive\n",
       "97745       97996  I'm not sure I agree. More Americans are consi...  positive\n",
       "97746       97997  If 40 billion could fix climate change why did...   neutral\n",
       "97747       97998  There are a lot of answers to climate change, ...   neutral\n",
       "97748       97999  He‚Äôs full of shit. It‚Äôs outlined and debunked ...  negative\n",
       "\n",
       "[97749 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_climate_df = pd.read_csv('en_climate_df.csv')\n",
    "en_climate_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16ffde6b",
   "metadata": {},
   "source": [
    "### Added from Deepansha ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50fdc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed variable names from Deepansha's original\n",
    "X = en_climate_df['text']\n",
    "Y = en_climate_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3987a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6d5dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9843d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countVectorizer.vocabulary_.get('climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d285930",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a58dd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cfb39ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36f74117",
   "metadata": {},
   "source": [
    "### Added from Raina ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac200634",
   "metadata": {},
   "source": [
    "### Test Reddit Climate Change Dataset Trained Model Using Other Datasets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5f9ff28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         family mormon have never tried explain them t...\n",
       "1        buddhism has very much lot compatible with chr...\n",
       "2        seriously don say thing first all they won get...\n",
       "3        what you have learned yours and only yours wha...\n",
       "4        for your own benefit you may want read living ...\n",
       "                               ...                        \n",
       "31643            coincidentally that how randia works too \n",
       "31644               here screen cap his live telecast jpg \n",
       "31645              shit forgot today date take upvote sir \n",
       "31646     fell for this good one also shows overtly opt...\n",
       "31647            thought now going ban 2000 rupee note lol\n",
       "Name: text, Length: 31648, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with cosmos redit sentiment data\n",
    "cosmos_reddit_test = pd.read_csv('en_cosmos_reddit_df.csv')\n",
    "cosmos_reddit_test_data = cosmos_reddit_test['text']\n",
    "cosmos_reddit_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8199ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'negative', ..., 'negative', 'positive',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_counts = countVectorizer.transform(cosmos_reddit_test_data)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e18044f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  12522\n",
      "passed:  0\n",
      "accuracy:  0.39566481294236605\n"
     ]
    }
   ],
   "source": [
    "cosmos_reddit_test_sentiment = cosmos_reddit_test['sentiment']\n",
    "\n",
    "correct = 0\n",
    "passed = 0\n",
    "for i in range(len(predicted)):\n",
    "    try:\n",
    "        if cosmos_reddit_test_sentiment[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    except KeyError:\n",
    "        passed += 1\n",
    "        pass\n",
    "print(\"correct: \", correct)\n",
    "print(\"passed: \", passed)\n",
    "print(\"accuracy: \", correct/(len(predicted)-passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "15530041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        when modi promised ‚Äúminimum government maximum...\n",
       "1        talk all the nonsense and continue all the dra...\n",
       "2        what did just say vote for modi  welcome bjp t...\n",
       "3        asking his supporters prefix chowkidar their n...\n",
       "4        answer who among these the most powerful world...\n",
       "                               ...                        \n",
       "91953                 india cant survive another term modi\n",
       "91954    modi hands down indians are too influenced bol...\n",
       "91955    rajdeep known congress supporters from day one...\n",
       "91956                       its bcoz they hate modi thats \n",
       "91957    narendra modi begin campaign trail address pub...\n",
       "Name: text, Length: 91958, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with cosmos twitter sentiment data\n",
    "cosmos_twitter_test = pd.read_csv('en_cosmos_twitter_df.csv')\n",
    "cosmos_twitter_test_data = cosmos_twitter_test['text']\n",
    "cosmos_twitter_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "874b1186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'negative', ..., 'positive', 'negative',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_counts = countVectorizer.transform(cosmos_twitter_test_data)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b0eeecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  32575\n",
      "passed:  0\n",
      "accuracy:  0.3542378042149677\n"
     ]
    }
   ],
   "source": [
    "cosmos_twitter_test_sentiment = cosmos_twitter_test['sentiment']\n",
    "\n",
    "correct = 0\n",
    "passed = 0\n",
    "for i in range(len(predicted)):\n",
    "    try:\n",
    "        if cosmos_twitter_test_sentiment[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    except KeyError:\n",
    "        passed += 1\n",
    "        pass\n",
    "print(\"correct: \", correct)\n",
    "print(\"passed: \", passed)\n",
    "print(\"accuracy: \", correct/(len(predicted)-passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "70604d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        What are we drinking today @TucanTribe \\n@MadB...\n",
       "1        Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
       "2        Worth reading while watching #WorldCup2022 htt...\n",
       "3        Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
       "4        If the BBC cares so much about human rights, h...\n",
       "                               ...                        \n",
       "21793    Leave #FIFA alone! Let the #soccer prevail and...\n",
       "21794    Three stars on this logo after WC\\n#WorldCup20...\n",
       "21795    What is really sickening is the west trying to...\n",
       "21796    Messi‚Äôs last World Cup! I am going with Argent...\n",
       "21797    Who will win the World Cup 2022?\\nüèÜ‚öΩÔ∏è\\n\\n@FIFA...\n",
       "Name: text, Length: 21798, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with fifa twitter data\n",
    "fifa_test = pd.read_csv('en_fifa_df.csv')\n",
    "fifa_test_data = fifa_test['text']\n",
    "fifa_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c022f717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'positive', ..., 'positive', 'positive',\n",
       "       'positive'], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new_counts = countVectorizer.transform(fifa_test_data)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdde8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  7799\n",
      "passed:  0\n",
      "accuracy:  0.3577851179007248\n"
     ]
    }
   ],
   "source": [
    "fifa_test_sentiment = fifa_test['sentiment']\n",
    "\n",
    "correct = 0\n",
    "passed = 0\n",
    "for i in range(len(predicted)):\n",
    "    try:\n",
    "        if fifa_test_sentiment[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    except KeyError:\n",
    "        passed += 1\n",
    "        pass\n",
    "print(\"correct: \", correct)\n",
    "print(\"passed: \", passed)\n",
    "print(\"accuracy: \", correct/(len(predicted)-passed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a047ad8",
   "metadata": {},
   "source": [
    "### Train on Different Datasets ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "480720d9",
   "metadata": {},
   "source": [
    "##### Twitter Sentiment Analysis #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193356f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised ‚Äúminimum government maximum...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>only modi theres question compare freebies wit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>modi disgrace always people who have expectati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>why want vote for modi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>why didnt people say this intolerance all beca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>modi body language has change lot seems like f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0      when modi promised ‚Äúminimum government maximum...         -1\n",
       "1      talk all the nonsense and continue all the dra...          0\n",
       "2      what did just say vote for modi  welcome bjp t...          1\n",
       "3      asking his supporters prefix chowkidar their n...          1\n",
       "4      answer who among these the most powerful world...          1\n",
       "...                                                  ...        ...\n",
       "99995  only modi theres question compare freebies wit...          0\n",
       "99996  modi disgrace always people who have expectati...          1\n",
       "99997                           why want vote for modi            0\n",
       "99998  why didnt people say this intolerance all beca...          0\n",
       "99999  modi body language has change lot seems like f...          0\n",
       "\n",
       "[99999 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmos_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb4d813f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised ‚Äúminimum government maximum...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>only modi theres question compare freebies wit...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>modi disgrace always people who have expectati...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>why want vote for modi</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>why didnt people say this intolerance all beca...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>modi body language has change lot seems like f...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99999 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      when modi promised ‚Äúminimum government maximum...  negative\n",
       "1      talk all the nonsense and continue all the dra...   neutral\n",
       "2      what did just say vote for modi  welcome bjp t...  positive\n",
       "3      asking his supporters prefix chowkidar their n...  positive\n",
       "4      answer who among these the most powerful world...  positive\n",
       "...                                                  ...       ...\n",
       "99995  only modi theres question compare freebies wit...   neutral\n",
       "99996  modi disgrace always people who have expectati...  positive\n",
       "99997                           why want vote for modi     neutral\n",
       "99998  why didnt people say this intolerance all beca...   neutral\n",
       "99999  modi body language has change lot seems like f...   neutral\n",
       "\n",
       "[99999 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmos_twitter_df[\"sentiment\"] = cosmos_twitter_df[\"sentiment\"].apply(lambda curr: \"negative\" if float(curr) < 0 else (\"positive\" if float(curr) > 0 else \"neutral\"))\n",
    "cosmos_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fe7cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cosmos_twitter_df['text']\n",
    "Y = cosmos_twitter_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8b6b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6229c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c15b1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30c62aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dd09d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e3b0088",
   "metadata": {},
   "source": [
    "##### Reddit Sentiment Analysis #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f831613",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_reddit_df[\"sentiment\"] = cosmos_reddit_df[\"sentiment\"].apply(lambda curr: \"negative\" if float(curr) < 0 else (\"positive\" if float(curr) > 0 else \"neutral\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "148d17fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37244</th>\n",
       "      <td>jesus</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37245</th>\n",
       "      <td>kya bhai pure saal chutiya banaya modi aur jab...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37246</th>\n",
       "      <td>downvote karna tha par upvote hogaya</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37247</th>\n",
       "      <td>haha nice</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37248</th>\n",
       "      <td>facebook itself now working bjp‚Äô cell</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37149 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0       family mormon have never tried explain them t...  positive\n",
       "1      buddhism has very much lot compatible with chr...  positive\n",
       "2      seriously don say thing first all they won get...  negative\n",
       "3      what you have learned yours and only yours wha...   neutral\n",
       "4      for your own benefit you may want read living ...  positive\n",
       "...                                                  ...       ...\n",
       "37244                                              jesus   neutral\n",
       "37245  kya bhai pure saal chutiya banaya modi aur jab...  positive\n",
       "37246              downvote karna tha par upvote hogaya    neutral\n",
       "37247                                         haha nice   positive\n",
       "37248             facebook itself now working bjp‚Äô cell    neutral\n",
       "\n",
       "[37149 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosmos_reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c035b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cosmos_reddit_df['text']\n",
    "Y = cosmos_reddit_df['sentiment']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=0.6)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "\n",
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdf40a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive', 'neutral', 'positive',\n",
       "       'negative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32143e5f",
   "metadata": {},
   "source": [
    "#### FIFA Dataset ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ff1bc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>And like, will the mexican fans be able to scr...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>Leave #FIFA alone! Let the #soccer prevail and...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>Three stars on this logo after WC\\n#WorldCup20...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>What is really sickening is the west trying to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>Messi‚Äôs last World Cup! I am going with Argent...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>Who will win the World Cup 2022?\\nüèÜ‚öΩÔ∏è\\n\\n@FIFA...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21787 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      What are we drinking today @TucanTribe \\n@MadB...   neutral\n",
       "1      Amazing @CanadaSoccerEN  #WorldCup2022 launch ...  positive\n",
       "2      Worth reading while watching #WorldCup2022 htt...  positive\n",
       "4      If the BBC cares so much about human rights, h...  negative\n",
       "5      And like, will the mexican fans be able to scr...  negative\n",
       "...                                                  ...       ...\n",
       "21995  Leave #FIFA alone! Let the #soccer prevail and...  positive\n",
       "21996  Three stars on this logo after WC\\n#WorldCup20...  positive\n",
       "21997  What is really sickening is the west trying to...  negative\n",
       "21998  Messi‚Äôs last World Cup! I am going with Argent...  positive\n",
       "21999  Who will win the World Cup 2022?\\nüèÜ‚öΩÔ∏è\\n\\n@FIFA...   neutral\n",
       "\n",
       "[21787 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_fifa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4eb91c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = en_fifa_df['text']\n",
    "Y = en_fifa_df['sentiment']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=0.6)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)\n",
    "\n",
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e653065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'neutral', 'positive', 'positive',\n",
       "       'negative', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
