{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63260db6",
   "metadata": {},
   "source": [
    "### Note on large .csv files ###\n",
    "The Reddit Climate Change Dataset (pavellexyr) and Sentiment140 (kazanova) are huge files with over 1 million observations. I cannot upload them to GitHub so here are the links to each one from Kaggle. There is also one dataset that is mentioned but has no associated code: Twitter US Airline Sentiment (crowdflower). This is included below if, for any reason, it is needed in the future.\n",
    "\n",
    "The Reddit Climate Change Dataset (pavellexyr): https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
    "\n",
    "Sentiment140 dataset with 1.6 million tweets (kazanova): https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "Twitter US Airline Sentiment (crowdflower): https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f65a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72d0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not having the limit breaks the kernel. This takes only the first 100K observations per dataset\n",
    "row_count = 1000\n",
    "max_obv = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450aa706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/langdetect/\n",
    "\n",
    "### uncomment this to install, then comment and restart kernel ###\n",
    "# %%capture\n",
    "# !pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "### regex for more lang checking\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fe043",
   "metadata": {},
   "source": [
    "### (pavellexyr) The Reddit Climate Change Dataset ###\n",
    "Only using the-reddit-climate-change-dataset-comments.csv which has a column for sentiment for the text. This file alone is over 4GB and processing all of it breaks the kernel so only the first 100K are processed. The columns of interest are 'body' for the comment's body text, and 'sentiment' for the analyzed sentiment per text. NaN values are dropped, leaving a little under 100K usuable observations. \n",
    "\n",
    "This is the only dataset that has continuous sentiment values [-1, 1] instead of discrete {-1, 0, 1} so if using ALL of these datasets together as one, it may be biased toward those discrete values than anything in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa0d0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rclimate_text = []\n",
    "rclimate_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('118Adatasets/the-reddit-climate-change-dataset-comments.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        rclimate_text += chunk['body'].tolist()\n",
    "        rclimate_sentiment += chunk['sentiment'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc3915c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rclimate_df = pd.DataFrame(data={'text': rclimate_text, 'sentiment': rclimate_sentiment})\n",
    "rclimate_df = rclimate_df.dropna()\n",
    "# rclimate_df.shape #expected (98422, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb035204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    if not line:\n",
    "        return np.nan\n",
    "    if line == \"\":\n",
    "        return np.nan\n",
    "    if not bool(line.strip()):\n",
    "        return np.nan\n",
    "    if len(line) < 1:\n",
    "        return np.nan\n",
    "    \n",
    "    if bool(re.match('^(?=.*[a-zA-Z])', line)):\n",
    "        try:\n",
    "            if detect(line) != 'en':\n",
    "                return np.nan\n",
    "        except LangDetectException:\n",
    "            return np.nan\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd7bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### expect 10 minutes to run for 100k rows\n",
    "### text_col is df['text']\n",
    "### sentiment_col is df['sentiment']\n",
    "### returns three lists of the same length\n",
    "\n",
    "def check_en(text_col, sentiment_col):\n",
    "    en_text = text_col.tolist()\n",
    "    en_sentiment = sentiment_col.tolist()\n",
    "    lang = []\n",
    "    \n",
    "    start = 0\n",
    "    for i in np.arange(row_count, len(en_text), row_count):\n",
    "        #observations <1000 at the end will be lost but impact is negligible\n",
    "        #!!!uncomment print statement below to show progress (recommended)!!!\n",
    "#         print(start, i)\n",
    "        lang += [validate_line(x) for x in en_text[start:i]]\n",
    "        start = i\n",
    "    print(\"Finished English check\")\n",
    "    ### all three return values should be of the same length\n",
    "    return en_text[0:len(lang)], en_sentiment[0:len(lang)], lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23cf4d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished English check\n"
     ]
    }
   ],
   "source": [
    "en_climate_text, en_climate_sentiment, en_climate_lang = check_en(rclimate_df['text'], rclimate_df['sentiment'])\n",
    "#all should be len 98000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2bc4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_climate_df = pd.DataFrame(data={'text': en_climate_text, 'sentiment': en_climate_sentiment, 'english': en_climate_lang})\n",
    "en_climate_df = en_climate_df.dropna()\n",
    "en_climate_df = en_climate_df.drop(columns=['english'])\n",
    "# print(en_climate_df.shape) #expected (97759, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee815f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_string(sentiment):\n",
    "    if type(sentiment) == int or type(sentiment) == float:\n",
    "        if sentiment < 0:\n",
    "            return \"negative\"\n",
    "        if sentiment > 0:\n",
    "            return \"positive\"\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a7b7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_climate_df['sentiment'] = en_climate_df['sentiment'].apply(sentiment_to_string)\n",
    "# en_climate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f0fd0",
   "metadata": {},
   "source": [
    "### (cosmos98) Twitter and Reddit Sentimental analysis Dataset ###\n",
    "Like with the dataset above, observations are limited to the first 100K and reduced to not have NaN values. Numeric values of {-1, 0, 1} are changed to {negative, neutral, positive} respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0adf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_twitter_text = []\n",
    "cosmos_twitter_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('118Adatasets/cosmos98_Twitter_Data.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        cosmos_twitter_text += chunk['clean_text'].tolist()\n",
    "        cosmos_twitter_sentiment += chunk['category'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cosmos_reddit_text = []\n",
    "cosmos_reddit_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('118Adatasets/cosmos98_Reddit_Data.csv', chunksize=row_count):\n",
    "    if i <max_obv:\n",
    "        cosmos_reddit_text += chunk['clean_comment'].tolist()\n",
    "        cosmos_reddit_sentiment += chunk['category'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1c3138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosmos_twitter_df = pd.DataFrame(data={'text': cosmos_twitter_text, 'sentiment': cosmos_twitter_sentiment}).dropna()\n",
    "cosmos_reddit_df = pd.DataFrame(data={'text': cosmos_reddit_text, 'sentiment': cosmos_reddit_sentiment}).dropna()\n",
    "# cosmos_twitter_df.shape #expected (99999, 2)\n",
    "# cosmos_reddit_df.shape #expected (37149, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f40db3",
   "metadata": {},
   "source": [
    "### (cosmos98) Twitter dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7c6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished English check\n"
     ]
    }
   ],
   "source": [
    "en_cosmos_twitter_text, en_cosmos_twitter_sentiment, en_cosmos_twitter_lang = check_en(cosmos_twitter_df['text'], cosmos_twitter_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c14db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91822, 2)\n"
     ]
    }
   ],
   "source": [
    "en_cosmos_twitter_df = pd.DataFrame(data={'text': en_cosmos_twitter_text, 'sentiment':en_cosmos_twitter_sentiment, 'english':en_cosmos_twitter_lang})\n",
    "en_cosmos_twitter_df = en_cosmos_twitter_df.dropna()\n",
    "en_cosmos_twitter_df = en_cosmos_twitter_df.drop(columns=['english'])\n",
    "print(en_cosmos_twitter_df.shape) #expected (91952, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a548ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98995</th>\n",
       "      <td>india cant survive another term modi</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98996</th>\n",
       "      <td>modi hands down indians are too influenced bol...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98997</th>\n",
       "      <td>rajdeep known congress supporters from day one...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98998</th>\n",
       "      <td>its bcoz they hate modi thats</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98999</th>\n",
       "      <td>narendra modi begin campaign trail address pub...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91822 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      when modi promised “minimum government maximum...  negative\n",
       "1      talk all the nonsense and continue all the dra...   neutral\n",
       "2      what did just say vote for modi  welcome bjp t...  positive\n",
       "3      asking his supporters prefix chowkidar their n...  positive\n",
       "4      answer who among these the most powerful world...  positive\n",
       "...                                                  ...       ...\n",
       "98995               india cant survive another term modi   neutral\n",
       "98996  modi hands down indians are too influenced bol...  negative\n",
       "98997  rajdeep known congress supporters from day one...   neutral\n",
       "98998                     its bcoz they hate modi thats   negative\n",
       "98999  narendra modi begin campaign trail address pub...   neutral\n",
       "\n",
       "[91822 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_cosmos_twitter_df['sentiment'] = en_cosmos_twitter_df['sentiment'].apply(sentiment_to_string)\n",
    "# en_cosmos_twitter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46bf5d2",
   "metadata": {},
   "source": [
    "### (cosmos98) Reddit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7439005f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished English check\n"
     ]
    }
   ],
   "source": [
    "en_cosmos_reddit_text, en_cosmos_reddit_sentiment, en_cosmos_reddit_lang = check_en(cosmos_reddit_df['text'], cosmos_reddit_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d02c0e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_cosmos_reddit_df = pd.DataFrame(data={'text': en_cosmos_reddit_text, 'sentiment': en_cosmos_reddit_sentiment, 'english':en_cosmos_reddit_lang})\n",
    "en_cosmos_reddit_df = en_cosmos_reddit_df.dropna()\n",
    "en_cosmos_reddit_df = en_cosmos_reddit_df.drop(columns=['english'])\n",
    "# print(en_cosmos_reddit_df.shape) #expected (31669, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e6016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_cosmos_reddit_df['sentiment'] = en_cosmos_reddit_df['sentiment'].apply(sentiment_to_string)\n",
    "# en_cosmos_reddit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b55ac1",
   "metadata": {},
   "source": [
    "### NOT USABLE -- (kazanova) Sentiment140 dataset with 1.6 million tweets -- NOT USABLE ###\n",
    "Dataset originally has 1.6 million tweets, limited to 100K. Dropped NaN values\n",
    "\n",
    "Unlike the previous datasets, sentiment is recorded as 0=negative, 2=neutral, 4=positive.\n",
    "However, the target column that is supposed to record this is entirely 0 which is unlikely for a set of 1.6 million tweets. Therefore, the sentiment from this dataset cannot be used with the others. It might be saved for a seperate purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97e54983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kazanova_text = []\n",
    "# kazanova_sentiment = []\n",
    "# #this dataset used the first observation as the columns\n",
    "# col_text = 5\n",
    "# col_sentiment = 0\n",
    "# i = 0\n",
    "# for chunk in pd.read_csv('118Adatasets/kazanova_sentiment140.csv', chunksize=row_count):\n",
    "#     if i < max_obv:\n",
    "#         kazanova_text += chunk.iloc[:,5].tolist()\n",
    "#         kazanova_sentiment += chunk.iloc[:,0].tolist()\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec0168a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kazanova_df = pd.DataFrame(data={'text':kazanova_text, 'sentiment':kazanova_sentiment})\n",
    "# kazanova_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd37f03",
   "metadata": {},
   "source": [
    "### NOT USABLE -- (crowdflower) Twitter US Airline Sentiment -- NOT USABLE ###\n",
    "Dataset has Tweet id for the text, but not the actual text itself. Sentiment is recorded in strings as 'positive', 'negative' and 'neutral' which can be converted to numerical values of 1, -1, and 0 respectively. There is no text readily available in the file so I will not write code for the numeric conversion here. Unlike the kazanova dataset, we likely cannot use this for another purpose without taking significant time to extract the text from the Tweet id for each observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbec67",
   "metadata": {},
   "source": [
    "### (tirendazacademy) FIFA World Cup 2022 Tweets ###\n",
    "Has Tweet text body and sentiment in strings of 'positive', 'negative' and 'neutral' which are unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e585da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_text = []\n",
    "fifa_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('118Adatasets/fifa_world_cup_2022_tweets.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        fifa_text += chunk['Tweet'].tolist()\n",
    "        fifa_sentiment += chunk['Sentiment'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a743e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_df = pd.DataFrame(data={'text':fifa_text,'sentiment':fifa_sentiment}).dropna()\n",
    "# fifa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baef7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished English check\n"
     ]
    }
   ],
   "source": [
    "en_fifa_text, en_fifa_sentiment, en_fifa_lang = check_en(fifa_df['text'], fifa_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "399f35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fifa_df = pd.DataFrame(data={'text': en_fifa_text, 'sentiment': en_fifa_sentiment, 'english': en_fifa_lang})\n",
    "en_fifa_df = en_fifa_df.dropna()\n",
    "en_fifa_df = en_fifa_df.drop(columns=['english'])\n",
    "# print(en_fifa_df.shape) #expected (21798, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d2860",
   "metadata": {},
   "source": [
    "Below are the cleaned DataFrames, having only the columns 'text' (unchanged) and 'sentiment' which has values \"positive\", \"negative\" or \"neutral\". The maximum number of observations per dataset is 100k rows, but any NaN values and text bodies that were not detected to be English (truncated to the nearest thousand) are dropped to make the data more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11f48a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_climate_df #shape (97759, 2)\n",
    "# en_cosmos_reddit_df #shape (31669, 2)\n",
    "# en_cosmos_twitter_df #shape (91952, 2)\n",
    "# en_fifa_df #shape (21798, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffde6b",
   "metadata": {},
   "source": [
    "### Added from Deepansha ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50fdc325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed variable names from Deepansha's original\n",
    "X = en_climate_df['text']\n",
    "Y = en_climate_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3987a630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6d5dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer = CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9843d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# countVectorizer.vocabulary_.get('climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d285930",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a58dd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cfb39ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'positive', 'positive', 'positive',\n",
       "       'negative', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
