{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63260db6",
   "metadata": {},
   "source": [
    "### Note on large .csv files ###\n",
    "The Reddit Climate Change Dataset (pavellexyr) and Sentiment140 (kazanova) are huge files with over 1 million observations. I cannot upload them to GitHub so here are the links to each one from Kaggle. There is also one dataset that is mentioned but has no associated code: Twitter US Airline Sentiment (crowdflower). This is included below if, for any reason, it is needed in the future.\n",
    "\n",
    "The Reddit Climate Change Dataset (pavellexyr): https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
    "\n",
    "Sentiment140 dataset with 1.6 million tweets (kazanova): https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "Twitter US Airline Sentiment (crowdflower): https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f65a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72d0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOT having the limit breaks the kernel. This takes only the first 100K observations per dataset\n",
    "row_count = 1000\n",
    "max_obv = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fe043",
   "metadata": {},
   "source": [
    "### (pavellexyr) The Reddit Climate Change Dataset ###\n",
    "Only using the-reddit-climate-change-dataset-comments.csv which has a column for sentiment for the text. This file alone is over 4GB and processing all of it breaks the kernel so only the first 100K are processed. The columns of interest are 'body' for the comment's body text, and 'sentiment' for the analyzed sentiment per text. NaN values are dropped, leaving a little under 100K usuable observations. \n",
    "\n",
    "This is the only dataset that has continuous sentiment values [-1, 1] instead of discrete {-1, 0, 1} so if using ALL of these datasets together as one, it may be biased toward those discrete values than anything in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa0d0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'the-reddit-climate-change-dataset-comments.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gr/fc2cy0q90y552xwg5gr_f8f80000gn/T/ipykernel_77761/1946384894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# file_directory = '118Adatasets/the-reddit-climate-change-dataset-comments.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'the-reddit-climate-change-dataset-comments.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_obv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mrclimate_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the-reddit-climate-change-dataset-comments.csv'"
     ]
    }
   ],
   "source": [
    "rclimate_text = []\n",
    "rclimate_sentiment = []\n",
    "i = 0\n",
    "# file_directory = '118Adatasets/the-reddit-climate-change-dataset-comments.csv'\n",
    "file_directory = 'the-reddit-climate-change-dataset-comments.csv'\n",
    "for chunk in pd.read_csv(file_directory, chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        rclimate_text += chunk['body'].tolist()\n",
    "        rclimate_sentiment += chunk['sentiment'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3915c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rclimate_df = pd.DataFrame(data={'text': rclimate_text, 'sentiment': rclimate_sentiment}).dropna()\n",
    "rclimate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f0fd0",
   "metadata": {},
   "source": [
    "### (cosmos98) Twitter and Reddit Sentimental analysis Dataset ###\n",
    "Like with the dataset above, observations are limited to the first 100K and reduced to not have NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0adf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_twitter_text = []\n",
    "cosmos_twitter_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('cosmos98_Twitter_Data.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        cosmos_twitter_text += chunk['clean_text'].tolist()\n",
    "        cosmos_twitter_sentiment += chunk['category'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cosmos_reddit_text = []\n",
    "cosmos_reddit_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('cosmos98_Reddit_Data.csv', chunksize=row_count):\n",
    "    if i <max_obv:\n",
    "        cosmos_reddit_text += chunk['clean_comment'].tolist()\n",
    "        cosmos_reddit_sentiment += chunk['category'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1c3138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cosmos_twitter_df = pd.DataFrame(data={'text': cosmos_twitter_text, 'sentiment': cosmos_twitter_sentiment}).dropna()\n",
    "cosmos_reddit_df = pd.DataFrame(data={'text': cosmos_reddit_text, 'sentiment': cosmos_reddit_sentiment}).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b55ac1",
   "metadata": {},
   "source": [
    "### (kazanova) Sentiment140 dataset with 1.6 million tweets -- not usable ###\n",
    "Dataset originally has 1.6 million tweets, limited to 100K. Dropped NaN values\n",
    "\n",
    "Unlike the previous datasets, sentiment is recorded as 0=negative, 2=neutral, 4=positive.\n",
    "However, the target column that is supposed to record this is entirely 0 which is unlikely for a set of 1.6 million tweets. Therefore, the sentiment from this dataset cannot be used with the others. It might be saved for a seperate purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97e54983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kazanova_text = []\n",
    "# kazanova_sentiment = []\n",
    "# #this dataset used the first observation as the columns\n",
    "# col_text = 5\n",
    "# col_sentiment = 0\n",
    "# i = 0\n",
    "# for chunk in pd.read_csv('118Adatasets/kazanova_sentiment140.csv', chunksize=row_count):\n",
    "#     if i < max_obv:\n",
    "#         kazanova_text += chunk.iloc[:,5].tolist()\n",
    "#         kazanova_sentiment += chunk.iloc[:,0].tolist()\n",
    "#         i += 1\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0168a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kazanova_df = pd.DataFrame(data={'text':kazanova_text, 'sentiment':kazanova_sentiment})\n",
    "# kazanova_df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd37f03",
   "metadata": {},
   "source": [
    "### (crowdflower) Twitter US Airline Sentiment -- not usable ###\n",
    "Dataset has Tweet id for the text, but not the actual text itself. Sentiment is recorded in strings as 'positive', 'negative' and 'neutral' which can be converted to numerical values of 1, -1, and 0 respectively. There is no text readily available in the file so I will not write code for the numeric conversion here. Unlike the kazanova dataset, we likely cannot use this for another purpose without taking significant time to extract the text from the Tweet id for each observation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbbec67",
   "metadata": {},
   "source": [
    "### (tirendazacademy) FIFA World Cup 2022 Tweets ###\n",
    "Has Tweet text body and sentiment in strings of 'positive', 'negative' and 'neutral' which can be converted to 1, -1, and 0 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3e585da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_text = []\n",
    "fifa_sentiment = []\n",
    "i = 0\n",
    "for chunk in pd.read_csv('fifa_world_cup_2022_tweets.csv', chunksize=row_count):\n",
    "    if i < max_obv:\n",
    "        fifa_text += chunk['Tweet'].tolist()\n",
    "        fifa_sentiment += chunk['Sentiment'].tolist()\n",
    "        i += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df98f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_num_sentiment(x):\n",
    "    if x.isnumeric():\n",
    "        return x\n",
    "    else:\n",
    "        if x == 'positive':\n",
    "            return 1\n",
    "        elif x == 'neutral':\n",
    "            return 0\n",
    "        elif x == 'negative':\n",
    "            return -1\n",
    "        else:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a743e2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>Here We go World cup 2022 #WorldCup2022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>Anderlecht confirms former Viborg FF's Jesper ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>Great thread to read before the start of #Worl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>Raphinha wants Brazil to be united at the #Wor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>How to buy $SOT on PinkSale?🤔\\n\\nHave you been...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22524 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0      What are we drinking today @TucanTribe \\n@MadB...          0\n",
       "1      Amazing @CanadaSoccerEN  #WorldCup2022 launch ...          1\n",
       "2      Worth reading while watching #WorldCup2022 htt...          1\n",
       "3      Golden Maknae shinning bright\\n\\nhttps://t.co/...          1\n",
       "4      If the BBC cares so much about human rights, h...         -1\n",
       "...                                                  ...        ...\n",
       "22519            Here We go World cup 2022 #WorldCup2022          1\n",
       "22520  Anderlecht confirms former Viborg FF's Jesper ...          0\n",
       "22521  Great thread to read before the start of #Worl...          1\n",
       "22522  Raphinha wants Brazil to be united at the #Wor...          1\n",
       "22523  How to buy $SOT on PinkSale?🤔\\n\\nHave you been...          0\n",
       "\n",
       "[22524 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifa_df = pd.DataFrame(data={'text':fifa_text,'sentiment':[str_num_sentiment(x) for x in fifa_sentiment]}).dropna()\n",
    "fifa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d2860",
   "metadata": {},
   "source": [
    "Below are the cleaned DataFrames, having only the columns 'text' (unchanged) and 'sentiment' which has values [-1, 1]. Only rclimate_df has continuous values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11f48a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rclimate_df\n",
    "# cosmos_reddit_df\n",
    "# cosmos_twitter_df\n",
    "# fifa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10c0d105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeah but what the above commenter is saying is...</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any comparison of efficiency between solar and...</td>\n",
       "      <td>-0.9877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm honestly waiting for climate change and th...</td>\n",
       "      <td>-0.1143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not just Sacramento. It's actually happening a...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think climate change tends to get some peopl...</td>\n",
       "      <td>0.6634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Yet despite that misinformation, Joe fucking P...</td>\n",
       "      <td>0.2411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>That's oversimplified. \\n\\nThird world countri...</td>\n",
       "      <td>-0.7080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>The net loss between Lake Powell and Lake Mead...</td>\n",
       "      <td>0.6882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Got news for ya. Democrats have been in power ...</td>\n",
       "      <td>-0.0521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>My point at the end was the others shouldn’t b...</td>\n",
       "      <td>-0.4767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98422 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0      Yeah but what the above commenter is saying is...     0.5719\n",
       "1      Any comparison of efficiency between solar and...    -0.9877\n",
       "2      I'm honestly waiting for climate change and th...    -0.1143\n",
       "3      Not just Sacramento. It's actually happening a...     0.0000\n",
       "4      I think climate change tends to get some peopl...     0.6634\n",
       "...                                                  ...        ...\n",
       "99995  Yet despite that misinformation, Joe fucking P...     0.2411\n",
       "99996  That's oversimplified. \\n\\nThird world countri...    -0.7080\n",
       "99997  The net loss between Lake Powell and Lake Mead...     0.6882\n",
       "99998  Got news for ya. Democrats have been in power ...    -0.0521\n",
       "99999  My point at the end was the others shouldn’t b...    -0.4767\n",
       "\n",
       "[98422 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rclimate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4827e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sentiment analysis supevised ML classifier on reddit climate change dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28bc01f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yeah but what the above commenter is saying is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Any comparison of efficiency between solar and...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm honestly waiting for climate change and th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not just Sacramento. It's actually happening a...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I think climate change tends to get some peopl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Yet despite that misinformation, Joe fucking P...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>That's oversimplified. \\n\\nThird world countri...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>The net loss between Lake Powell and Lake Mead...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Got news for ya. Democrats have been in power ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>My point at the end was the others shouldn’t b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98422 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0      Yeah but what the above commenter is saying is...  positive\n",
       "1      Any comparison of efficiency between solar and...  negative\n",
       "2      I'm honestly waiting for climate change and th...  negative\n",
       "3      Not just Sacramento. It's actually happening a...   neutral\n",
       "4      I think climate change tends to get some peopl...  positive\n",
       "...                                                  ...       ...\n",
       "99995  Yet despite that misinformation, Joe fucking P...  positive\n",
       "99996  That's oversimplified. \\n\\nThird world countri...  negative\n",
       "99997  The net loss between Lake Powell and Lake Mead...  positive\n",
       "99998  Got news for ya. Democrats have been in power ...  negative\n",
       "99999  My point at the end was the others shouldn’t b...  negative\n",
       "\n",
       "[98422 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rclimate_df[\"sentiment\"] = rclimate_df[\"sentiment\"].apply(lambda curr: \"negative\" if float(curr) < 0 else (\"positive\" if float(curr) > 0 else \"neutral\"))\n",
    "rclimate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0348dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do more data cleaning here, and remove not good words !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f506948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the x's and y's\n",
    "X = rclimate_df[\"text\"]\n",
    "Y = rclimate_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310dfec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=15, train_size=.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d354e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "countVectorizer= CountVectorizer()\n",
    "X_train = countVectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c48cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23281"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countVectorizer.vocabulary_.get('climate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24fb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
    "X_train = tf_transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec050ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(3).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8999612",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = ['yayyy!', 'terrible', \"she walked to the right\", \"woohoo\", \"I don't feel good\", \"sad\", \"feel kinda blue\"]\n",
    "X_new_counts = countVectorizer.transform(tester)\n",
    "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03c44bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddbb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
