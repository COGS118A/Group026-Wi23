{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Deepansha Singh\n",
    "- Raina Song\n",
    "- Ilya Kogan\n",
    "- Winah Ruiz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Natural language processing (NLP) has been making headlines all over the news in recent months due to the release and gaining popularity of ChatGPT (chatbot developed by OpenAI).\n",
    "To take a closer look at the field of NLP, we implement a sentiment analysis, a NLP technique that is used to determine the text attitude on a scale from -1 to 1 where absolute positive input has score 1, absolute negative input has score of -1, and a score of 0 is for neutral sentiment.\n",
    "In this project we examine how general a sentiment analysis tool can be \n",
    "(e.g. when trained on data on a specific topic how well it can perform on other topics that it didn’t encounter before or general topicless data) \n",
    "by using different sampling techniques \n",
    "(k-fold Cross Validation and Nested Cross Validation) \n",
    "with different classifiers \n",
    "(Support Vector Classifier, k-Nearest Neighbors algorithm, Decision Trees, etc.) \n",
    "and quantifying their performance using various error metrics \n",
    "(precision, recall, or accuracy scores).\n",
    "We will be using social media data from Reddit with sentiment labels to train our models, choose the best one according to an error metric, and then test how well our “best” model performs on a new dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In previous studies, the generalisability and interoperability of NLP models in tasks such as fake news detection<a name=\"fakenews\"></a>[<sup>[1]</sup>](#fakenews)<a name=\"fakenews2\"></a>[<sup>[2]</sup>](#fakenews2), hate speech detection<a name=\"hatespeech\"></a>[<sup>[3]</sup>](#hatespeech), and sentiment analysis<a name=\"sentian\"></a>[<sup>[4]</sup>](#sentian) were explored. It was found that generalisability is a challenge for NLP algorithms, as they generalize poorly on new and unseen datasets. However, robust NLP models are desired to perform large-scale, cross-categorical classification tasks on the internet or social media platforms. Some studies suggest that the biases found in models may due to the small datasets the models are trained on, which make the models prone to overfitting issues. Larger and more diverse datasets are required to reduce biases in trained models. It was suggested that cross-dataset testing is a useful tool to evaluate model generalisability performance realistically. Due to such reasons, it motivates our project to explore the generalisability of NLP sentiment analysis models on different datasets pertaining to different subject matter.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "We are performing one of the techniques to quantify text, sentiment analysis, using different classifiers. We are then examining how well our model trained on The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate) can perform on Twitter and Reddit Sentimental Analysis Dataset<a name=\"twitterredditsentian\"></a>[<sup>[6]</sup>](#twitterredditsentian), Twitter US Airline Sentiment<a name=\"twitterusairline\"></a>[<sup>[7]</sup>](#twitterusairline), and FIFA World Cup 2022 Tweets<a name=\"fifatweets\"></a>[<sup>[8]</sup>](#fifatweets) by using different error metrics (precision, recall, or accuracy scores) and how does a choice of error metric change what is the best model (i.e. whether we have a completely best model or not)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The main dataset we are planning to use is The Reddit Climate Change Dataset<a name=\"redditclimate\"></a>[<sup>[5]</sup>](#redditclimate), containing all mentions of climate change on Reddit before September 1, 2022. The dataset can be downloaded from Kaggle (2GB). The dataset contains 4.6 million unique values (observations) in total, and around 10 usable variables. \n",
    "\n",
    "An observation in this dataset consists of:\n",
    "- Type of the data point (if it’s valid or not)\n",
    "- A unique id of the data point\n",
    "- Unique subreddit (i.e. category) id of the data point\n",
    "- Human-readable name of the data point’s category\n",
    "- If the reddit comment (the data point) is NSFW (Not Safe For Work)\n",
    "- Timestamp of the comment’s creation\n",
    "- Permalink to the comment on Reddit\n",
    "- The comment’s body text\n",
    "- Analyzed sentiment for the comment (value between -1 and 1)\n",
    "- The comment’s score\n",
    "\n",
    "Some critical variables include the comment’s body text and the analyzed sentiment for each comment. Using these variables we can process the body text using NLP and sentiment analysis algorithms and verify the outputs we generate against the “ground truth” labels, i.e. the analyzed sentiment for each comment in the dataset. \n",
    "\n",
    "Most comments in the dataset are in string format that can be directly used. However, some comments include special characters such as emojis that would need to be removed before being used in our models. Moreover, columns that are not needed, such as the permalink of the comment, can be removed to declutter the dataset. \n",
    "\n",
    "After training our models with this dataset, we are planning to use some other similar datasets that contain social media posts or comments and their analyzed sentiment to test how well the model can generalise. Some of these additional datasets we considered include Twitter and Reddit Sentimental Analysis Dataset<a name=\"twitterredditsentian\"></a>[<sup>[6]</sup>](#twitterredditsentian), Twitter US Airline Sentiment<a name=\"twitterusairline\"></a>[<sup>[7]</sup>](#twitterusairline), and FIFA World Cup 2022 Tweets<a name=\"fifatweets\"></a>[<sup>[8]</sup>](#fifatweets), all from Kaggle. All of these datasets contain more than 7,000 unique observations, along with the post or comment body text and the analyzed sentiment. However, the datasets all use their own scales for sentiment. For example, some are in the scale of -1 to 1, some are 0 to 1 and some are strings “positive” and “negative”. We therefore need to normalize these data points to make sure the data in the datasets is comparable with our output.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We have considered 3 different approaches for the proposed solution. For approaches 1 and 2, we are looking at supervised machine learning methods for the classifiers. And for approach 3, which is beyond the scope of the class (but if we have extra time) we will also do some exploration for some deep learning package’s accuracy on the dataset also.\n",
    "\n",
    "**Approach #1 : Supervised ML, using scikit packages to take care of BOTH the data preprocessing and also the NLP feature extraction.**\n",
    "\n",
    "All the preprocessing will be taken care of by the CountVectorizer.\n",
    "Please note that we are following this tutorial from the Scikit documentation <a name=\"scikit1\"></a>[<sup>[9]</sup>](#scikit1). Additionally, please note that since we haven’t previously worked with natural language processing problems in class/lecture before, we are  following this tutorial’s methods for feature extraction. Additionally, we did a lot of thorough, extensive research on various research paper’s approaches for extracting features for text input. Some features <a name=\"scikit1\"></a>[<sup>[9]</sup>](#scikit1)<a name=\"featextract\"></a>[<sup>[10]</sup>](#featextract) we would be looking at are bag-of-words feature, tf-idf frequencies <a name=\"wiki\"></a>[<sup>[11]</sup>](#wiki), and word embeddings <a name=\"mediium\"></a>[<sup>[12]</sup>](#medium). \n",
    "\n",
    "After extracting all the various features, the next step would be to run the various classifiers <a name=\"scikit2\"></a>[<sup>[13]</sup>](#scikit2) on the training dataset. Specifically, these are the following classifiers we are planning on running on these features: Support Vector Classifier (as discussed in class), K-nearest neighbors  (as discussed in class), and Decision trees (as discussed in class).\n",
    "Some new classifiers, which we haven’t learned in class yet, which we are also planning to investigate in further detail are neural net, Naive Bayes, Ada Boost, etc. <a name=\"scikit2\"></a>[<sup>[13]</sup>](#scikit2). \n",
    "\n",
    "Additionally, for the training sampling techniques, we will be using the following techniques and do comparisons between the different methods that we learned in class: K-fold validation with training, with training/validation/testing, and Nested cross validation. \n",
    "<br>\n",
    "\n",
    "**Approach #2: Supervised ML, doing data preprocessing and above feature extraction from scratch.**\n",
    "\n",
    "The steps for this approach will be similar to the first approach, however for the data preprocessing we will be doing everything from scratch instead of using CountVectorizer. \n",
    "Specifically, these are some of the data preprocessing techniques that we have researched about <a name=\"empstudy\"></a>[<sup>[14]</sup>](#empstudy)<a name=\"featextractreview\"></a>[<sup>[15]</sup>](#featextractreview) for natural language processing sub-field of machine learning are lemmatization/stemming, tokenization, and looking at part of speech for text.\n",
    "Additionally, we will be looking at more research papers and trying to see more data preprocessing techniques for text input data.\n",
    "<br>\n",
    "\n",
    "**Approach #3 (Above & Beyond, Extra work outside of class algorithms): Running on pre-trained neural networks.**\n",
    "\n",
    "Along with supervised ML approaches, there has been a lot of extensive, thorough research being done in deep learning space for Natural Language Processing. Google’s BERT and Open AI’s GPT-3 <a name=\"top8\"></a>[<sup>[16]</sup>](#top8) are just two of the popular NLP deep learning models out there. \n",
    "We will be loading these pretrained models in the Pytorch library. This is a library we haven’t learned in class, but is widely used in deep learning research problems. Specifically, this is new Machine learning content, in deep learning and transformers space, and we spent time understanding these really cool new ML concepts (links below). \n",
    "After performing the above tasks for the supervised machine learning component of the project, we will also be comparing the accuracy of this model to various pretrained models from HuggingFace for sentiment analysis, and see which model provides best accuracy, specifically BERT pretrained model <a name=\"huggingface1\"></a>[<sup>[17]</sup>](#huggingface1) and Chat GPT-2 pretrained model <a name=\"huggingface2\"></a>[<sup>[18]</sup>](#huggingface2).\n",
    "\n",
    "If time permits, we will also try to train a neural network from scratch for sentiment analysis and do further research on research papers in this space. \n",
    "\n",
    "For all these different classification models, we will see what results they produce on the unlabelled dataset and see how accurate they seem on the unlabeled dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We will be using various different error metrics such as precision, recall, and overall accuracy score to evaluate how well those classifiers can generalize in sentiment analysis given different subject matter, and if there is a significant difference in performance between each classifier.\n",
    "Precision and recall will also be used to judge how sensitive our classifiers are to negative and positive sentiments, providing insight as to what pitfalls they are subject to in the human language such as sarcasm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the datasets all coming from real people collected through social media, there are ethical concerns over their consent in being part of this project as points of data, even with their words and personal information accessible through the media the data is extracted from (Twitter and Reddit).\n",
    "Proper data cleaning and handling would require giving those people as much anonymity that is possible to be given, such as removing their usernames/handles and even the permalink/id to avoid being traced back to a user or community.\n",
    "\n",
    "The goal of this project is to see if, given the labels already assigned in some datasets, how well can a model be trained on them to label other datasets?\n",
    "As the data comes from a sample of people in the population of those respective media for certain topics, the text content may be biased towards/against certain topics or communities.\n",
    "Some datasets have already included sentiment labels to indicate if they are positive, negative, or neutral.\n",
    "Using this data in training may contribute to the model’s perceived connotation of those and related texts in unlabeled datasets.\n",
    "Conclusions drawn from those results may be harmful to a degree regardless of actual intent or content because of the nature of applying sentiment analysis.\n",
    "This project and those that are working with it do not show any personal support or opposition for any of the data gathered, and to keep the integrity of the project itself, all usable content (those in the English language) can be used without censorship as to not influence the direction of the model toward our personal preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Communication channel is FB Messenger.\n",
    "* Notify of any changes of plans in the communication channel as soon as possible.\n",
    "* Confirm or reschedule meeting times for upcoming week if you know that you will not be able to meet at the specified time. \n",
    "* Come prepared to meetings with completed tasks.\n",
    "* Ask for assistance anytime you feel like you need it.\n",
    "* Don’t be afraid to pair up to learn some new skill together or help out one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date & Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/22 @2 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/26 @2 PM |  Import and wrangle Data, do some EDA | Review/edit wrangling; discuss further steps | \n",
    "| 3/3 @2 PM  | Read reviews of our project  | Tailor our project based on what we learned from peer reviews; discuss further steps   |\n",
    "| 3/8 @2 PM  | Prepare your part for checkpoint submission | Resolve any issues, submit Checkpoint; discuss further steps   |\n",
    "| 3/12 @2 PM | Be up to date, bring in any concerns or what still needs to be done. | Review of what is done so far. Discuss further steps |\n",
    "| 3/16 @2 PM | Complete your part(s). Prepare any questions and/or concerns. Last chance to change something| Discuss last steps towards project completion. Distribute work to complete the project |\n",
    "| 3/22 @2 PM | Finish all the gaps, fill in all the nits and bits  | Turn in the Project  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"fakenews\"></a>1.[^](#fakenews): LHoy, N., &amp; Koulouri, T. (2022). Exploring the generalisability of fake news detection models. *2022 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata55660.2022.10020583<br> \n",
    "<a name=\"fakenews2\"></a>2.[^](#fakenews2): Blackledge, C., &amp; Atapour-Abarghouei, A. (2021). Transforming fake news: Robust generalisable news classification using Transformers. *2021 IEEE International Conference on Big Data (Big Data)*. https://doi.org/10.1109/bigdata52589.2021.9671970<br>\n",
    "<a name=\"hatespeech\"></a>3.[^](#hatespeech): Yin W, Zubiaga A. 2021. Towards generalisable hate speech detection: a review on obstacles and solutions. *PeerJ Computer Science* 7:e598 https://doi.org/10.7717/peerj-cs.598\n",
    "<br>\n",
    "<a name=\"sentian\"></a>4.[^](#sentian): Moore, A., & Rayson, P. (2018). Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for target dependent sentiment analysis. *arXiv preprint arXiv:1806.05219*.\n",
    "<br>\n",
    "<a name=\"redditclimate\"></a>5.[^](#redditclimate): https://www.kaggle.com/datasets/pavellexyr/the-reddit-climate-change-dataset\n",
    "<br>\n",
    "<a name=\"twitterredditsentian\"></a>6.[^](#twitterredditsentian): https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n",
    "<br>\n",
    "<a name=\"twitterusairline\"></a>7.[^](#twitterusairline): https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment\n",
    "<br>\n",
    "<a name=\"fifatweets\"></a>8.[^](#fifatweets): https://www.kaggle.com/datasets/tirendazacademy/fifa-world-cup-2022-tweets\n",
    "<br>\n",
    "<a name=\"scikit1\"></a>9.[^](#scikit1): https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#exercise-2-sentiment-analysis-on-movie-reviews\n",
    "<br>\n",
    "<a name=\"featextract\"></a>10.[^](#featextract): Zareapoor, M., &amp; K. R, S. (2015). Feature extraction or feature selection for text classification: A case study on phishing email detection. *International Journal of Information Engineering and Electronic Business*, 7(2), 60–65. https://doi.org/10.5815/ijieeb.2015.02.08 \n",
    "<br>\n",
    "<a name=\"wiki\"></a>11.[^](#wiki): https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "<br>\n",
    "<a name=\"medium\"></a>12.[^](#medium): https://medium.com/geekculture/text-feature-extraction-3-3-word-embeddings-model-e98f3d270dce\n",
    "<br>\n",
    "<a name=\"scikit2\"></a>13.[^](#scikit2): https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "<br>\n",
    "<a name=\"empstudy\"></a>14.[^](#empstudy): Sun, X., Liu, X., Hu, J., &amp; Zhu, J. (2014). Empirical studies on the NLP techniques for source code data preprocessing. *Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies*. https://doi.org/10.1145/2627508.2627514 \n",
    "<br>\n",
    "<a name=\"featextractreview\"></a>15.[^](#featextractreview): Asghar, M. Z., Khan, A., Ahmad, S., & Kundi, F. M. (2014). A review of feature extraction in sentiment analysis. *Journal of Basic and Applied Scientific Research*, 4(3), 181-186.\n",
    "<br>\n",
    "<a name=\"top8\"></a>16.[^](#top8): https://analyticsindiamag.com/top-8-pre-trained-nlp-models-developers-must-know/\n",
    "<br>\n",
    "<a name=\"huggingface1\"></a>17.[^](#huggingface1): https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis?text=Yay%21%21\n",
    "<br>\n",
    "<a name=\"huggingface2\"></a>18.[^](#huggingface2): https://huggingface.co/michelecafagna26/gpt2-medium-finetuned-sst2-sentiment?text=yayy%21%21\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
